\documentclass[twoside,openright]{uva-bachelor-thesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}

%\usepackage[dutch]{babel}  % uncomment if you write in dutch
\usepackage{graphicx}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[style=ieee]{biblatex}
\addbibresource{thesis.bib}
\usepackage{lmodern}
%Enable for final compilation
%\usepackage[stretch=10]{microtype}
\usepackage{graphicx}
\usepackage{varioref} % Fancy references
\usepackage{caption} % Fixes hyperref to figure
\usepackage[toc,acronym,xindy]{glossaries}
\usepackage{xspace}
\usepackage{nameref}
\usepackage{cleveref}
\usepackage[binary-units]{siunitx}

\makeglossaries{}

\newcommand{\betternewacronym}[3]{\newacronym{#1}{#2}{#3}
\expandafter\newcommand\csname #1\endcsname{\gls{#1}\xspace}
\expandafter\newcommand\csname #1up\endcsname{\Gls{#1}\xspace}
\expandafter\newcommand\csname #1long\endcsname{\acrlong{#1}\xspace}
}


\betternewacronym{abs}{ABS}{adaptive bitrate streaming}
\betternewacronym{iss}{ISS}{ISS Smooth Streaming}
\betternewacronym{dash}{DASH}{Dynamic Adaptive Streaming over HTTP}
\betternewacronym{hds}{HDS}{HTTP Dynamic Streaming}
\betternewacronym{hls}{HLS}{HTTP Live Streaming}
\betternewacronym{usp}{USP}{Unified Streaming}
\betternewacronym{cdn}{CDN}{content delivery network}
\betternewacronym{ipp}{IPP}{IsmProxyPass}
\betternewacronym{lt}{LT}{late transmuxing}


% Title Page
\title{Late Transmuxing:\\Improving caching \\in video streaming}
\author{Jelte Fennema}
\supervisors{Dirk Griffioen (Unified Streaming), Robert Belleman (UvA)}
\signedby{Robert Belleman (UvA)}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}


\tableofcontents

\chapter{Introduction}
% Start with the grand scheme of things with respect to VOD
The internet started out as a means of sending messages from one place to
another. Since then it has become much faster and is used by many different
applications. It is still used to send messages, but because storage became
cheaper and the internet became faster, it is now used for applications that
require way more bandwith. Because of it a revolution in the way we watch videos
has taken place. Movie theaters and cable TV are not the only ways to watch
movies and TV programs anymore. Streaming of videos over the internet has grown
to an enormous industry that can be used to watch any kind of video imaginable.
Lots of amature videos are shared using YouTube, movies and TV shows can be
watched easily using Netflix and even live TV is being moved to the internet,
for instance by the BBC iPlayer.

This way of video streaming is not limited to normal computers. It is done on on
lots of different devices, like smartphones, tablets, set-top boxes,
mediacenters or Smart TVs. Especially on mobile devices the network conditions
can suddenly change drastically, for instance when changing the network
connection from WiFi to a mobile network. With initial streaming technologies
these changes caused buffering issues when the client was trying to get a high
quality video using a low bandwith network connection. This brought life to the
notion of \abs, which is a way of streaming video that changes the quality
of the video that is being watched, based on what the network connection can
handle \autocite{abs}.

There are a couple of streaming solutions that implement \abs, most of
those use HTTP as the underlying protocol \autocite{http}.  One of the reasons
for this is that HTTP is already in heavy and this way it is possible to
leverage the already existing infrastructure that is in place for other HTTP
communication. A disadvantage of HTTP however is that it is a protocol for
sending whole files and entire video files are typically quite large. This is
why the solutions stream little sections of the video instead of the whole file.
This means that the original video file should be cut in smaller files. They
also need to store some extra metadata in those files, so a player will actually
know how to play it. The solutions all have their own (slightly) different way
of adding this metadata.

% Maybe start with usp here and tell about the problem they solve.
When hosting video these different formats can cause quite a bit of trouble.
The reason for this is that the different formats are not supported equally by
the players used by different devices or browsers. Because of this, a good video
host should have its videos available in a lot of different formats to make sure
they can be viewed by every device and browser. The most obvious way of doing
this ofcourse, is generating the files in all the needed formats and then
setting up a file server that simply serves all the little files. This works but
takes up quite a bit of space since the same video is essentially stored
multiple times.

\Gls{usp} is a company that provides an easy to use solution for this problem.
They do this by supplying modules for a set of popular HTTP servers, Apache,
Nginx, IIS and Lighttpd. The way their module solves the problem of storing the
video multiple times is by only storing the mp4 and converting that on the fly
to the requested format. At the moment four different \gls{abs} formats are
supported by the \gls{usp} software. \Gls{iss}, which is developed by Microsoft and is
used in Silverlight based players~\autocite{iss}. \Gls{hds}, which is developed by
Adobe and is used in Flash based players \autocite{hds}.  \Gls{hls}, which is developed
by Apple and used on the iPhone, iPad and Apple TVs~\autocite{hls}. The last
format is \gls{dash}, which is developed as an ISO standard and is used in
\textsc{HTML5} video players~\autocite{dash}.


\section{Traditional setups}
% Maybe less details
The simplest way on the fly conversion can be used in a setup, is again a
single server, that converts the mp4 to the needed format for every request.
However, in large streaming setups this is not enough for two reasons. The first
one is that the storage server might be far away from the viewer, which means
more time is spent waiting before watching a video. The second one is that the
storage server can be easily overwhelmed with requests, because its just one
server.

Using a \gls{cdn} with reverse proxy caching nodes is a simple solution to these
problems, which can also used when hosting lots of content besides
videos~\autocites{cdns}{revproxy}. Those nodes forward the request to the
storage server and cache the response. If another request comes for the same
content a node will just respond with the response in its cache. This works
quite good, since a small percentage of the videos account for a large
percentage of the views.
% For live streaming this also works fantastic since it's not even a whole video
% that is downloaded a lot, it is only the last part of that video.

The big problem of this setup is that, just like with the very first setup, it
stores the converted files instead of the raw files, only now in the cache
instead of the storage server, which has two downsides in this case. Again, a
server will essentially contain the same content multiple times, only in this
case the cache server. The other downside, specific to this setup, is that the
cache will also request basically the same content from the storage. Those
requests mean more internal traffic and more waiting time for the viewer.

\section{Approach}
% Very short exlaination of the idea to solve the problem.
The problem described above is why this thesis will explore the possibility of
using a ``Late Transmuxing''. This setup would do the conversion of the video
directly on the caching nodes, instead of at the storage level. This means that
the cache server would request segments of the mp4 file from the storage server,
instead of the already converted files. This approach should solve both the
extra storage problem and the internal traffic problem, because only the segments
of mp4 have to be stored and requested.






\chapter{Problem analysis}\label{ch:problem}
% Explain the normal setups in detail together with their shortcommings.
The introduction briefly described some of the server setups used for streaming
video. This chapter will explain in detail what kind of setups are currently
available and what their advantages and disadvantages are. The setups that will
be looked into are the ones that convert an mp4 on the fly to different
\gls{abs} formats. The reason for this is that those setups already have a clear
advantage over servers that simply store files in all formats, namely the space
required to store the videos. To explain how each of the setups work, each step
will be described, from the client requesting a segment of video in a specific
format, to the client receiving that segment.

The different server setups consist of one or two servers. One server, which is
present in every setup, stores the the mp4 files on disk, this will be called
the storage server. The more advanced setups also contain a second server, which
serves as a reverse proxy, this server will be called the proxy server. In the
setups where a proxy server is used the client connects to the proxy server and
the proxy server then requests content from the storage server that it needs
to fulfill the request from the client. When caching takes place on the proxy
server, it can also be called the cache server.

Lastly, on both of these physical servers will run an HTTP server to serve the
HTTP requests. For illustration purposes a clear distinction between a
physical server and the HTTP server that runs on that server needs to be made.
This will be done by describing the setups as if Apache is used on the storage
server and Nginx is used on the proxy server. Keep in mind however that this is
just for illustration purposes, in practice any of the HTTP servers supported by
\gls{usp} can be used on any of the physical servers.


\section{The single server setup}
This setup was already briefly explained in the introduction and it is the most
simple setup that will be covered. In this setup a client requests a segment
directly from the storage server. Apache will then locate the mp4 that contains
the requested segment. It will then read the parts of the file that are
necessary to create the segment in the requested format. From these parts, it
will then create the segment and send it back to the client. After the client
receives it, it can be played by the player. For a schematic overview of this
setup see figure~\vref{fig:singleserver}.

\begin{figure}[h]
    \includegraphics{singleserver_diagram.pdf}
    \caption{The single server setup}\label{fig:singleserver}
\end{figure}

The main advantage of this setup is that is very simple to set up. It is just
one server and all it has to do store files and run the conversion code. The
main disadvantage is also that it is just one server. This means that it can
easily be overwhelmed with requests when traffic is high, especially since it
not only has to serve files, but also run the conversion code. Another issue
that occures because it is just a single server is that the time to reach the
client can be quite long when the client is located far away from the server,
for instance on another continent.

%The last disadvantage is that it scales very
%badly, since it is impossible to just add extra servers without duplicating the
%storage of all the videos to those servers as well. All that can be done is
%upgrading the existing server and this can only be done until a certain limit.


\section{The CDN setup}
Another setup that was already briefly introduced is the \gls{cdn} setup. It is
a quite a simple extension of the single server setup. In this setup the proxy
server will receive the request for the segment from the client. It then passes
that exact reqest along to the storage server. The storage server will then
generate the segment from the mp4 and send the segment back to the proxy server.
The proxy server will then send the response back to the client, but it will
also cache the response. That way, when a client requests the same segment in
the same format the proxy server will be able to serve the segment directly from
its cache, instead of requesting it from the storage server. For a schematic
overview of this setup see figure~\vref{fig:cdn}.

\begin{figure}[h]
    \includegraphics{cdn_diagram.pdf}
    \caption{The \gls{cdn} setup}\label{fig:cdn}
\end{figure}

Using this setup solves the issues present in the single server
setup. The simple server setup was easy to overload with lots of requests.
Caching should help quite a lot in this regard, since there normally is a set of
videos that receives a lot of views and a set that receives (almost) none.
Because of the caching, the storage only needs to convert the mp4 for the
popular videos once for each segment and format, instead of for every request.
The latency issue is also mostly mittigated by the caching. By simply adding
more proxy servers, spread around the globe, only the first request for a
segment will have high latency, the next request for that same segment only
needs to travel to the proxy server.

One of the disadvantages of this setup is ofcourse that it is less trivial to
set up than the single server setup. It also has some other disadvantages.
Because it caches the converted segments, it is essentially storing the same
part of the video multiple times, only in another format, which is exactly the
problem that on the fly conversion was there to solve. This time it is only
doing this however for the most frequently watched videos, so it is less of an
issue, but it is still something that is best kept to a minimum.

Another small disadvantage is caused by the proxy requests this server uses. A
request that is not cached yet will actually take longer to return to the
client than it would with the single server setup. This is because of the
overhead of first traveling to the proxy server and then to the storage server.
This is almost always a longer (and never a shorter) route then the route
directly to the storage server. Because this longer route is taken only the
first time a specific segment is requested and the next times the route will be
shorter this is not such a big problem, but it still is something that should be
noted.


\section{The IsmProxyPass setup}
Another setup that the \gls{usp} software supports is the \gls{ipp} setup. Just
like the \gls{cdn} setup, this setup extends the single server setup. However it
does this in quite a different way. The key difference is that the \gls{usp}
software does not run on the storage server, but on the proxy server. This
time, when the proxy server receives a request from a client, instead of passing
it directly to the storage, it will send a series of HTTP range requests, which
are generated by the \gls{usp} software, to the storage
server~\autocite{rangerequests}. Range requests are a type of HTTP requests that
do not request the whole file that the server would return for a normal HTTP
request, it only requests a specific byte range of that file.  These kind of
requests are especially useful when only a small piece of a large file is
needed.

\begin{figure}[h]
    \includegraphics{ismproxy_diagram.pdf}
    \caption{The IsmProxyPass setup}\label{fig:ismproxy}
\end{figure}

Upon receiving one of those range requests, the storage server will simply
return the requested bytes from the mp4. When the proxy server then receives
those bytes it will use them to generate the segment requested by the client.
After this is done the segment will be returned to the client. This all works by
using \glspl{usp} custom \ipplong server directive and supply it a URL at
which it can reach the storage server~\autocites{uspismproxy}. For a schematic overview of this setup
see figure~\vref{fig:ismproxy}.

This setup shares some of the advantages of the \gls{cdn} setup. It can easily
be used to mitigate the storage overloading issue. By just adding more proxy
servers the CPU intensive conversion work can be shared by multiple servers. All
that the storage server has to do is serve the byte ranges that the proxy
servers need, which is not nearly as intensive as the conversion. The latency
issue however, is still present. Since for every request of a client still needs
to reach the location of the storage server, it does not matter that they are
converted to range requests, they still need reach the storage eventually.

The issue with internal requests is again present and it is much worse than with
the \gls{cdn} setup. The first reason for this is that no caching takes place,
so every clients request has to travel further, instead of just the first
request for each segment. Another issue is that every request of a client spawns
multiple range requests, which means even more overhead and thus more waiting.

Because of all this, it seems that this setup is only worse than the \gls{cdn}
setup. However, it does have one important advantage. Hosting large files on
dedicated servers or VPSâ€™s can be quite expensive. This is why there are
services that specialize just in hosting files, like Amazon~S3. Services like
this, which will be called dumb storage from now on, are quite popular for
hosting large files, like videos, because it is relatively cheap. Dumb storage
allows files to be uploaded and it will make them accessible over HTTP by
supplying a URL\@.  However, there is now way to access the server internals,
like the HTTP server that is running on the server. This is no problem for
static content, however, on the fly conversion makes the videos dynamic content.
The \gls{usp} software needs to be installed into the HTTP server itself, which
means it is impossible to use on dumb storage. This setup solves this issue by
separating the storage and the conversion tasks, which makes it possible to host
the mp4 files on dumb storage and use other servers for the conversion.

\section{Combining the setups}
Both the \gls{cdn} setup and the \gls{ipp} setup have their clear advatages.
Both their disadvantages are clear as well. A combination between the \gls{cdn}
setup and the \gls{ipp} setup that would have both of their advantages could be
an ideal setup. However, currently no such setup exists. The clear advantage
of a combinition like this would be that it is possible to combine cheap dumb
storage with the speed of \gls{cdn} setup. However, another advantage is gained
as well. In this combination the proxy server would be caching the range
requests of the raw mp4 instead of the request for a converted segment. This
would mean that the same content is not stored multiple times in the cache, like
in the \gls{cdn} setup, since the same cached range requests can be used to
generate a segment in a different format. This would also mean that the amount
of internal traffic would go down since no requests would have to be done for
the same segment in a different format. These advantages are why this thesis
sets out to design such a setup and to test its performance against currently
available setups.







\chapter{Design considerations}\label{ch:design}
% Explain the search for technologies used instead of stating the decision and
% then explaining why.
To build the proposed setup, some design decisions have be made. Most of those
decisions concern the technologies to be used, but there are some other ones as
well. This chapter will describe the options to choose from. It will also
explain what the final decision for those choices is and why.


\section{HTTP servers}
The proposed setup consists of two different servers, the storage server and the
proxy server. Both of these servers need to have an HTTP server to handle the
incomming requests. Nginx and Apache are both very capable and popular HTTP
servers. They are also both available for Linux and they are supported by the
\gls{usp} software. However, there are quite a different in some aspects and some
of those aspects are important for the proposed setup.


\subsection{General differences}
There are a couple of general differences between Apache and Nginx that have
already been thoroughly explored in a multitude of
resources~\autocites{nginx1}{nginx2}{apachevsnginx}{nginxperformancescale}.
Their core architecture is very different. Apache spawns a new thread for each
connection, while Nginx dispatches the requests to already existing threads.
This means that Nginx can handle a lot more concurrent requests than Apache with
the same resources, since spawning threads is quite expensive resource wise.
Their take on dynamic content is also quite different. Apache uses modules that
can be easily loaded into Apache itself to handle dynamic content.  Nginx just
receives the requests and sends them to other applications if content needs to
be dynamically generated, in some setups Apache is even one of those other
applications. These differences are why Apache shines as an application server,
but Nginx is usually a good choice for serving as a static file server or as a
reverse proxy.


\subsection{USP integration}
While USP supports both servers fully, because of the architecture of both
servers it is easier to create a working setup with Apache. Apache can load
modules for dynamic functionality easily, so this is used by the \gls{usp}
software~\autocite{uspapache}. Just download and install the module
and enable it by adding some lines to the server config files. Nginx however is
not designed to just load modules on the fly. For dynamic content it normally
just contacts an external application. However, it is possible to add new
functionality to Nginx itself by including a module at compile time. This is
what needs to be done to install the \gls{usp} software for Nginx. This means
it is not possible to just use the Nginx supplied by the Linux distribution of
choice. This all makes it quite clear that the installation process for Apache
is quite a bit easier than the one for Nginx.


\subsection{Caching}
Since the proposed setup depends heavily on caching in the reverse proxy, it is
important that the server application does this wel. Both Nginx and Apache have
good caching support~\autocites{nginxcache}{apachecache}. There is a key
difference however, this is the better cache locking support in Nginx. Cache
locking is a mechanism that can be used to reduce load on the storage server.
Normally if a request comes in for some content and there is no cache for it,
it will be proxied through to the storage server. This happens even if another
request for the same content is already being proxied through but has not
retured yet. If cache locking is enabled, the second request for some content
will wait until the first request has returned and fills the cache. This can
significantly reduce the load on the storage server, since the second request
can just fetch the response from the cache. Nginx and Apache both support cache
locking. However, Apache only uses it as a hint and it will still fetch from the
storage if it receives a request with a header enable that tells it to do so.
Nginx will always make a second request wait if cache locking is enabled, even
with special headers.


\subsection{Scripting}
% In server lua vs CGI
Both servers allow for low level scripting using
Lua~\autocites{nginxlua}{cloudflarelua}{apachelua}. This can be very useful if
complicated logic is needed to handle a request correctly. One big difference
between the Apache version and the Nginx version is that the Nginx version
can spawn subrequests, which can be very useful. For instance, one of the things
subrequests allow, is that based on some checks in the Lua code the request can
be handled by two entirely different Nginx paths. This way one can use the power
of the complicated lua logic in combination with the simpleness of normal
configurations.


\subsection{The decision}
Nginx is chosen as the HTTP server for the reverse proxy in the proposed setup.
The better support for cache locking and scripting are the main reasons, since
those could be useful for the actual setup. Its low memory footprint and speed
are nice sideeffects, but they are not essential for the setup itself.  Which
type of HTTP server is used by the storage server is not really important for
this specific setup, since they can both serve static files and support
range requests out of the box. Apache will be chosen for the setup that will be
tested, not because it is better for this setup, but because it is easier to
enable the \gls{usp} software for it. This will make it easier to test against
other setups that need on the fly conversion on the storage server, like the
\gls{cdn} setup.




\section{What responses to cache}
The proposed setup is basically a version the \gls{ipp} setup with
caching enabled. Because of this, it is important to decide what it is that
needs to be cached exactly. There are two types of responses that could be
cached on the proxy server, the range request responses it receives from the
storage server and the converted segments it sends to the client. The
range requests responses are most important to cache. These are most important
since they cause network traffic between the proxy server and the storage
server, which introduces latency.

The responses the proxy server sends to the client might also be useful to
cache. In the \gls{cdn} setup these responses were mostly cached because of
network latency. However, when already caching the range requests in the
\gls{ipp} setup, network latency shouldn't be an issue since the responses can
be generated on the fly from the cached range requests. This generating however
is CPU intensive, so caching the result of this conversion should reduce the CPU
load. Caching the converted segments still has the storage disadvantage it had
with the \gls{cdn} setup. The same segments are essentially cached multiple
times and this is even worse when also caching the range requets, since that
means segments are cached one extra time in comparison to the \gls{cdn} setup.


\subsection{The decision}
Currently it is unclear what the effect of caching each type of response has on
the performance of the setup. This is why the final implementation will come in two
flavors. A setup where all of the previously described responses are cached and
one where only the range requests are cached. A third flavor would be one where
only the responses to the client are cached, but since this flavor would
basically be the same as the \cdn setup it is not necessary when also testing
against that setup.




\section{Caching range requests}\label{sec:cachingrange}
% Maybe use subsections so it will be less of a wall of text.
Since the \ipp setup uses range requests it is also important that those
can be cached. This is not directly supported in Nginx or Apache. The reason for
this is that it is not exactly trivial to do. Most caches work by supplying it
with some data to cache, together with a key by which the data can be retrieved later
on.

Ideally the content would be cached using the key of the full file and some info
would be saved about what range is actually cached. Then when another request
for a range comes in it would check in the cache if some of the requested bytes
are in there. If not all of them are there it should request just the bytes that
are not cached from the upstream server and then add those to the same cache
entry for future use. A cache that works like this hasn't been implemented by
Apache or Nginx and quite possible by no HTTP server in existence. However, our
proposed setup is not the first that would benefit from caching range requests.
So some suboptimal solutions have been proposed for different use cases.

\subsection{Ignore range}
The most simple one, which is employed by Nginx by default, is removing the
range request header from the request when sending it upstream. This will
request the whole file instead of just the requesting byte range. Nginx will
then cache this and return the whole file everytime a range is requested.
According to the RFC this behaviour is allowed, but this it is clearly not
desired behaviour, since the whole point of range requests is that requesting
the full file is not necessary.

\subsection{Match exact range}
A simple solution to this, proposed on the Nginx forums, is to force Nginx to
request the range from upstream and then adding the requested range to the cache
key~\autocite{nginxcacheforum}. This basically means that a range request is
handled by the cache as if it were a seperate URL\@. The main downside of this
solution is that when overlapping byte ranges are requested they will both be
cached in full.  This means this  solution works fine when the ranges that get
requested are always the same. However if the requested ranges are different
almost every time, this solution is not acceptable since a lot of the cache wil
contain the same bytes, because of the overlap in the byte ranges.

% Check wat proxy_force_ranges doet en of het te gebruiken is in mijn versie van
\subsection{Ranger}
Ranger extends further on the previous solution and tries to solve the overlap
problem~\autocite{ranger}. Everytime a range request comes in, instead of
sending it directly to the upstream server, it generates other range requests
that together contain all the bytes requested. These new range requests always
have the same block size and they also start at the same byte position.  These
requests will then be sent to upstream and after a response comes back they will
be cached instead of the original request.  This way, even though the initial
range request can be any range and can overlap with others, the requests sent to
upstream will always match previously sent requests, which makes sure that cache
hits occur every time part of a range is requested that was requested before,
even though it's not exactly the same. There are only two small issues with this
approach.  The first one is that more data is requested from the upstream server
than is actually needed to fulfill the request. However, this is probably not
too severe when the block size is chosen small enough. The second issue is that
when large ranges are requested, a lot of requests need to be send to the
upstream server, because the large request gets split into a lot of smaller
ones. Which could mean some unnecessary overhead.

\subsection{The decision}
At this point it is clear that the first approach for caching range requests is
not sufficient, since that would mean the whole video would have to be sent to
the proxy server to send a small segment of that video to the client. The second
aproach is one that shows great promise for the proposed setup. If the timing
and length of segments sent for each format are the same, it would probably mean
that the ranges requested are exactly the same as well. This would mean that the
caching with this approach would just work. However, if after preliminary
testing it is shown that a lot of the ranges are (slightly) different, using
ranger might be preferable over the second approach.











\chapter{Implementation}
The implementation of the \gls{lt} setup consists of configuration files for the
HTTP servers running on the proxy server and the storage server. What it
implements, stating it very simply, is a version of the \gls{ipp} setup with
caching enabled. This chapter will explain at a high level how the
implementation works. The implementation described is the flavor that caches all
types of responses, not just the range requests. The reason for this is that the
other flavor can simply be achieved by disabling the caching of the responses to
the client.


\section{Initial implementation}
With everything from \nameref{ch:design} in mind the initial \lt setup was
implemented in the way described here. The setup should consist of two servers,
a proxy server that runs the \usp code and a storage server that serves byte
ranges. The proxy server should be able to cache the responses it sends to the
client and the responses it receives from the storage server. The \usp code does
not support caching natively. This can easily be solved by letting Nginx make
requests to itself. It can then use its normal proxy and caching mechanisms to
pass requests on to the actual destination and cache the responses it gets
before returning those to the original requester.

A schematic overview of the setup described next can be found in figure
\vref{fig:latetransold}. First, client requests a segment from the proxy server.
Nginx receives this request and passes it through its caching proxy to the
\ipplong handler. This handler is setup to request the byte ranges from a second
proxy running in Nginx. It then uses the \usp software to generate those range
requests and requests them from that proxy. The second proxy then passes those
requests on to the storage server. The storage server will then receive the
requests and return the requested byte ranges.

When the second proxy in Nginx receives a response to one of its range requests
it caches that response. This is done by using the second method described in
section~\vref{sec:cachingrange}. Apart from caching the response the proxy will
send it back to the \ipplong handler. Apache will then generate the originally requested
segment and return that back to Nginx.  Nginx will then cache the segment and
return it to the client.


\begin{figure}
    \includegraphics{latetransold_diagram.pdf}
    \caption{The initial late transmuxing setup}\label{fig:latetransold}
\end{figure}

The outcome of the initial tests of this setup were as expected. As it turns out
however, in its current form the setup has one big problem. When testing this
setup with more concurrent connections than the amount of threads that
Nginx is defined to use it results in deadlock. The reason for this is that the
\ipplong directive supplied by \usp spawns the range requests in a blocking way,
instead of in the non blocking way that Nginx normally spawns requests. This
causes the threads that spawn them to wait until they get a response from the
upstream server. However, since the upstream server is the same server, it will
only be able to do so when not all the threads are blocked. When using enough
concurrent client requests all available threads will get into the waiting
state, so no more threads are available to serve the requests that would get
them out of this state.

\section{Final implementation}
There are a couple of possible solutions to the deadlock problem, but by far the
easiest is moving the blocking code outside Nginx. In the final implementation
this is done by not only running Nginx on the proxy server, but also Apache. The
only task of Apache is to run the \ipplong code. This way all that Nginx does is
proxying and caching. Once a request of a client comes in, it will pass
through Nginx to Apache on the same server. Apache will then generate the range
requests and those will pass through Nginx again, so it can cache their
responses. Nginx will then send them to the storage server and then the response
from the storage will follow the chain back. And just like in the initial
implementation when a response passes a proxy in Nginx it will be cached. For a
schematic overview of this setup see figure \vref{fig:latetrans}.


\begin{figure}
    \includegraphics{latetrans_diagram.pdf}
    \caption{The final late transmuxing setup}\label{fig:latetrans}
\end{figure}







\chapter{Experiments}
This chapter will explain the experiments that have been done to compare the
\lt setup to other available setups. It will also show the results of these
experiments.

\section{Setup}

\subsection{Virtual machines}
To create the proposed setup a client, a proxy server and a storage server are
needed. This could be have been done by using actual physical servers for every
type of machine. However, to make the development process easier the different
machines where simulated using virtual machines. VirtualBox was used to create
the VMs. One VM was used as the proxy and a second one was used as the storage
server. The host operating system was used as the client. Since the machines
have to communicate with eachother, a separate network interface was set up to
allow communication. The VMs have the same virtual hardware, they are both
allowed to use \SI{2}{\giga\byte} of RAM and one CPU core.

\subsubsection{Comcast}
Since the VMs will be running on the same machine there are almost no network
limitations. That is why artificial network throtteling is needed to simulate
more realistic real world environments. Comcast is a command line tool that
provides a simple interface over standard Linux tools to throttle the
network~\autocite{comcast}. Using Comcast the bandwidth between the proxy server
and the storage server is throttled down to allow a maximum of
\SI[per-mode=symbol]{100}{\mega\bit\per\second}.


\subsection{The different setups}
The VMs are six different setups that will be tested in a variety of tests.
These setups can be split into old setups and new setups and controll setups.
The old setups consist of of the \cdn and \ipp setup, described in
\nameref{ch:problem}. The new setups consist of the two flavors of the \lt
setup. The one that only caches the responses to the range requsts will be
called \lt-single and the one that also caches the responses to the client
\lt-double. The control setups are setups that are purely there to test the
effect of the actual caching the \lt and the \cdn setups. These setups have all
caching disabled, but the rest is the same and they will respectively be called
\lt-nocache \cdn-nocache.


\subsection{The tests}
The performance of each setup will be measured when downloading all the
segments of a whole video in the highest quality available in all the formats
supported by the \usp software. All the segments of the video in a specific
format will be refered to from now on as ``the video"

\subsubsection{Generating requests}
Since only one client is used, it is necessary that it generates a signifcant
amount of requests in a short time so it can be measured how the different
setups perform under different loads. Wrk is a HTTP benchmarking tool that will
be used to do this~\autocite{wrk}. It can be supplied with an URL, a duration
and the amount of concurrent connections. It will then fire requests for the
duration provided to that URL and afterwards it will output statistics about

Wrk also provides a scripting interface that allows for more complicated logic
for the requests to be sent. This interface is used by the testing setup to fire
requests for all the segments of the video. This is done by looping over the
contents of a file that contain all the urls for the different segments of the
video in a specific format. The files for the different formats have been
generated by letting a player play a the video from start to end and saving the
requests. The player used for this was the Unified Capture tool created by \usp,
which impersonates a player to save an online video to a file.

\subsubsection{Measuring performance}
Performance of a setup will be measured by a six kinds of characteristics. The
first three characteristics are simply measured by wrk. The first of these is
the amount of requests that are handled per second. The second is the amount of
Megabytes received per second and the third is the average time it takes for a
request to return a response. The fourth characteristic is the storage space the
cache directory takes up on the proxy server. The last two characteristcs
involve the traffic between the proxy server and the storage server. That
traffic is measured in the amount of requests and the amount of bytes sent.
These are simply measured by parsing the log files on the storage server.

\subsubsection{Cache conditions}
Performance tests will be executed on the different setups in three different
cache conditions. The first one is where the cache is cold. The second one is
where the cache is already warm from downloading the video in the same format
that is getting requested. The last one is where the cache is warm from
downloading the same video but in another format than the one that is requested.

The tests for the first and the last situation simply try to download the video
once, as fast as possible. The tests for the second situation try to download
the video as often as possible in a 30 second time frame. The reason this
difference exists, is the first and last situation both change to the second
situation when the video has been downloaded once, since the cache has become
warm from downloading the video in that format.

Since the different cache conditions have no impact on the setups without
caching, the \ipp, the \cdn-nocache and the \lt-nocache setup have only been
executed for the first situation. This is done to reduce the time that is needed
for a test run, so more useful tests can be executed in the same time.



\section{Results}
\subsection{Cold cache}
\subsubsection{Transfer speed measurements}
In the figures \ref{fig:first_time_mbps} and
\ref{fig:first_time_requests_per_second} a couple of things should be noticed.
\begin{enumerate}
    \item
        The \cdn type setups perform best in almost all of the the transfer
        speed measurements.

    \item
        All setups eventually reach a maximum transfer speed when the amount of
        concurrent connections is increased.

    \item
        The \cdn type setups already reach a maximum in transfer speed already
        after two connections.

    \item
        Unlike the other setups, the \ipp setup reacts very irregularly to
        an increasing amount of connections.

    \item
        Some setups behave quite similar to others. The \cdn type setups perform
        roughly the same and the caching \lt setups do this as well. The \ipp
        and the \lt-nocache setup share the maximum they reach and also perform
        quite the same with one connection. They differ in between the setups
        however.

    \item
        Most setups perform similarly for the different formats when comparing
        MB/s, but when comparing requests per second they perform worse for \hds
        and \hls than for \dash and \iss.

\end{enumerate}

\input{plots/first_time_mbps.tex}
\input{plots/first_time_requests_per_second.tex}
\subsubsection{Latency}
In figure 1 can be seen how lat
When looking at the graphs for the latency it can be seen that larger differences
start to occur when the amount of connections increase.
\input{plots/first_time_latency_mean.tex}
\subsubsection{Server measurements}
\input{plots/first_time_cache_usage.tex}
\input{plots/first_time_internal_mb.tex}
\input{plots/first_time_internal_requests.tex}
\clearpage

\subsection{Cache filled with the same format}
\subsubsection{Client measurements}
\input{plots/second_time_mbps.tex}
\input{plots/second_time_requests_per_second.tex}
\input{plots/second_time_latency_mean.tex}
\subsubsection{Server measurements}
\input{plots/second_time_cache_usage.tex}
\input{plots/second_time_internal_mb.tex}
\input{plots/second_time_internal_requests.tex}

\subsection{Cache filled with another format}
\subsubsection{Client measurements}
\input{plots/after_other_mbps.tex}
\input{plots/after_other_requests_per_second.tex}
\input{plots/after_other_latency_mean.tex}
\subsubsection{Server measurements}
\input{plots/after_other_cache_usage.tex}
\input{plots/after_other_internal_mb.tex}
\input{plots/after_other_internal_requests.tex}


\chapter{Discussion}
\section{Future Work}
\chapter{Conclusions}


\printbibliography[heading=bibintoc]{}

\printglossaries{}


\end{document}
