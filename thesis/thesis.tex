\documentclass[twoside,openright]{uva-bachelor-thesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}

%\usepackage[dutch]{babel}  % uncomment if you write in dutch
\usepackage{graphicx}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[style=ieee]{biblatex}
\addbibresource{thesis.bib}
\usepackage{lmodern}
%Enable for final compilation
%\usepackage[stretch=10]{microtype}
\usepackage{graphicx}
\usepackage{varioref}
\usepackage[toc,acronym,xindy]{glossaries}

\makeglossaries{}


\newacronym{abs}{ABS}{adaptive bitrate streaming}
\newacronym{iss}{ISS}{ISS Smooth Streaming}
\newacronym{dash}{DASH}{Dynamic Adaptive Streaming over HTTP}
\newacronym{hds}{HDS}{HTTP Dynamic Streaming}
\newacronym{hls}{HLS}{HTTP Live Streaming}
\newacronym{usp}{USP}{Unified Streaming}
\newacronym{cdn}{CDN}{content delivery network}
\newacronym{ipp}{IPP}{IsmProxyPass}


% Title Page
\title{Late Transmuxing:\\Improving caching \\in video streaming}
\author{Jelte Fennema}
\supervisors{Dirk Griffioen (Unified Streaming), Robert Belleman (UvA)}
\signedby{Robert Belleman (UvA)}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}


\tableofcontents

\chapter{Introduction}
% Start with the grand scheme of things with respect to VOD
The internet started out as a means of sending messages from one place to
another. Since then it has become much faster and is used by many different
applications. It is still used to send messages, but because storage became
cheaper and the internet became faster, it is now used for applications that
require way more bandwith. It supplied the means for the revolution in the way
we watch videos. Movie theaters and cable TV are not the only ways to watch
those anymore. Streaming of videos over the internet has grown to an enormous
industry of can be used to watch any video imaginable. Lots of amature videos
are shared using YouTube, movies and TV shows can be watched easily using
Netflix and even live TV is being moved to the internet, for instance by the BBC
iPlayer.

This way of video streaming is not limited to normal computers. It is done on on
lots of different devices, like smartphones, tablets, set-top boxes,
mediacenters or Smart TVs. Especially on mobile devices the network conditions
can suddenly change drastically, for instance when changing the network
connection from WiFi to a mobile network. With initial streaming technologies
these changes caused buffering issues when the client was trying to get a high
quality video using a low bandwith network connection. This brought life to the
notion of \gls{abs}, which is a way of streaming video that changes the quality
of the video that is being watched, based on what the network connection can
handle \autocite{abs}.

There are a couple of streaming solutions that implement \gls{abs}, most of
those use HTTP as the underlying protocol \autocite{http}.  One of the reasons
for this is that HTTP is already in heavy and this way it is possible to
leverage the already existing infrastructure that is in place for other HTTP
communication. A disadvantage of HTTP however is that it is a protocol for
sending whole files and entire video files are typically quite large. This is
why the solutions stream little sections of the video instead of the whole file.
This means that the original video file should be cut in smaller files. They
also need to store some extra metadata in those files, so a player will actually
know how to play it. The solutions all have their own (slightly) different way
of adding this metadata.

% Maybe start with usp here and tell about the problem they solve.
When hosting video these different formats can cause quite a bit of trouble.
The reason for this is that the different formats are not supported equally by
the players used by different devices or browsers. Because of this, a good video
host should have its videos available in a lot of different formats to make sure
they can be viewed by every device and browser. The most obvious way of doing
this is ofcourse generating the files in all the needed formats and then setting
up a file server that simply serves all the little files. This works but takes
up quite a bit of space since the same video is essentially stored multiple
times.

\Gls{usp} is a company that provides an easy to use solution for this problem.
They have modules for a set of popular HTTP servers, Apache, Nginx, IIS and
Lighttpd. The way their module solves the problem of storing the video multiple
times is by only storing the mp4 and converting that on the fly to the requested
format. At the moment four different \gls{abs} formats are supported by the
\gls{usp} software. \Gls{iss} is developed by Microsoft and is used in
Silverlight based players \autocite{iss}. \Gls{hds} is developed by Adobe and is
used in Flash based players \autocite{hds}.  \Gls{hls} is developed by Apple and
used on the iPhone, iPad and Apple TVs \autocite{hls}. The last format is
\gls{dash}, which is developed as an ISO standard and is used in \textsc{HTML5}
video players \autocite{dash}.


\section{Traditional setups}
% Maybe less details
The simplest way on the fly conversion can be used in a setup, is again a
single server, that converts the mp4 to the needed format for every request.
However, in large streaming setups this is not enough for two reasons. The first
one is that the storage server might be far away from the viewer, which means
more time is spent waiting before watching a video. The second one is that the
storage server can be easily overwhelmed with requests, because its just one
server.

Using a \gls{cdn} with reverse proxy caching nodes is a simple solution to these
problems \autocites{cdns}{revproxy}, which can also used when hosting lots of
content besides videos. Those nodes forward the request to the storage server
and cache the response. If another request comes for the same content a node
will just respond with the response in its cache. This works quite good, since a
small percentage of the videos account for a large percentage of the views.
% For live streaming this also works fantastic since it's not even a whole video
% that is downloaded a lot, it is only the last part of that video.

The big problem of this setup is that, just like with the very first setup, it
stores the converted files instead of the raw files, only now in the cache
instead of the storage server, which has two downsides in this case. Again, a
server will contain the same content multiple times, only in this case the cache
server. The other downside, specific to this setup, is that the cache will also
request basically the same content from the storage. Those requests mean more
internal traffic and more waiting time for the viewer.

\section{Approach}
% Very short exlaination of the idea to solve the problem.
The problem described above is why this thesis will explore the possibility of
using a ``Late Transmuxing''. This setup would do the conversion of the video
directly on the caching nodes, instead of at the storage level. This means that
the cache server would request segments of the mp4 file from the storage server,
instead of the already converted files. This approach should solve both the
extra storage problem and the internal traffic problem, because only the segments
of mp4 have to be stored and requested.






\chapter{Problem analysis}
% Explain the normal setups in detail together with their shortcommings.
As explained shortly in the introduction, when caching video segments, a segment
has to be cached for each supported format. This chapter will explain in detail
what kind of setups are currently available and what their advantages and
disadvantages are. The setups that will be looked into are ones that convert an
mp4 on the fly to different \gls{abs} formats. The reason for this is that those
setups already have a clear advantage over servers that simply store files in
all formats, namely the space required to store the videos. To explain how each
of the setups work, each step will be described, from the client requesting a
segment of video in a specific format, to the client receiving that segment.

The different server setups consist of one or two servers. One server, which is
present in every setup, stores the the mp4 files on disk, this will be called
the storage server. The more advanced setups also contain a second server, which
serves as a reverse proxy, this server will be called the proxy server. In the
setups where a proxy server is used the client connects to the proxy server and
the proxy server then requests content from the storage server that it needs
to fulfill the request from the client. When caching takes place on the proxy
server, it can also be called the cache server.

Lastly, on both of these physical servers will run a HTTP server to serve the
HTTP requests. For illustration purposes a clear distinction between a
physical server and the HTTP server that runs on that server needs to be made.
This will be done by describing the setups as if Apache is used on the storage
server and Nginx is used on the proxy server. Keep in mind however that this is
just for illustration purposes, in practice any of the HTTP servers supported by
\gls{usp} can be used on any of the physical servers.


\section{The single server setup}
This setup was already briefly explained in the introduction and it is the most
simple setup that will be covered. A client requests a segment directly from the
storage server. Apache will then locate the mp4 that contains the requested
segment. It will then read the parts of the file that are necessary to create
the segment in the requested format. From these parts, it will then create the
segment and send it back to the client. After the client receives it, it can be
played by the player. For a schematic overview of this setup see
figure~\vref{fig:singleserver}.

\begin{figure}[h]
    \includegraphics{singleserver_diagram.pdf}
    \caption{The single server setup}\label{fig:singleserver}
\end{figure}

The main advantage of this setup is that is very simple to set up. It is just
one server and all it has to do store files and run the conversion code. The
main disadvantage is also that it is just one server. This means that it can
easily be overwhelmed with requests when traffic is high, especially since it
not only has to serve files, but also run the conversion code. Another issue
that occures because it is just a single server is that the time to reach the
client can be quite long when the client is located far away from the server,
for instance on another continent.

%The last disadvantage is that it scales very
%badly, since it is impossible to just add extra servers without duplicating the
%storage of all the videos to those servers as well. All that can be done is
%upgrading the existing server and this can only be done until a certain limit.


\section{The CDN setup}
Another setup that was already briefly introduced is the \gls{cdn} setup. It is
a quite a simple extension on the single server setup. In this setup the proxy
server will receive the request for the segment from the client and passes that
exact reqest along to the storage server. The storage server will then generate
the segment from the mp4 and send the segment back to the proxy server.  The
proxy server will then send the response back to the client, but it will also
cache the response. That way, when a client requests the same segment in the
same format the proxy server will be able to serve the segment directly from its
cache, instead of requesting it from the storage server. For a schematic
overview of this setup see figure~\vref{fig:cdn}.

\begin{figure}[h]
    \includegraphics{cdn_diagram.pdf}
    \caption{The \gls{cdn} setup}\label{fig:cdn}
\end{figure}

Using this setup solves the issues present in the single server
setup. Because segments will be cached once they have been requested once,
overloading of the storage server is much less of an issue. This should help
quite a lot, since there probably is a set of videos that receives a lot of
views and a set that receives (almost) none. Because of the caching, the storage
only needs to convert the mp4 for the popular videos once for each segment and
format, instead of for every request.
% This is also why it scales pretty good,
% since the load on the storage server is never extremely high, just upgrading
% that server when nessecary is probably fine.
The latency issue is also mostly
mittigated by the caching. By simply adding more proxy servers, spread around
the globe, only the first request for a segment will have the high latency, the
next request for that same segment only need to travel to the proxy server.

One of the disadvantages of this setup is ofcourse that it is less trivial to
set up than the single server setup. It also has some other disadvantages,
because it is caching the converted segments, it is essentially storing the same
part of the video multiple times only in another format, which is exactly the
problem that on the fly conversion was there to solve. This time it is only
doing this however for the most frequently watched videos, so it is less of an
issue, but it's still something that is best kept to a minimum.

Another small disadvantage is caused by the proxy requests this server uses. A
request that isn't cached yet will actually take longer to return to the client
than it would with the single server setup. This is because of the overhead of
first traveling to the proxy server and then to the storage server. This is
almost always a longer route then the route directly to the storage server.
Because this longer route is taken only the first time a segment is requested
and the next times the route will be shorter this is not such a bad problem, but
it still is something that should be noted.


\section{The IsmProxyPass setup}
Another setup that the \gls{usp} software supports is the \gls{ipp} setup. Just
like the \gls{cdn} setup, this setup extends the single server setup. However it
does this in quite a different way. One of the key differences is that the
\gls{usp} software does not run on the storage server, but on the proxy server.
This time, when the proxy server receives a request from a client, instead of
passing it directly to the storage, it will send a series of HTTP range
requests, which are generated by the \gls{usp} software, to the storage
server~\autocite{rangerequests}. Range requests are a type of
HTTP requests that do not request the whole file that the server would return
for a normal HTTP request, it only requests a specific byte range of that file.
These kind of requests are especially useful when a small piece of a large file
is needed.

Upon receiving one of those range requests, the storage server will simply
return the requested bytes from the mp4. When the proxy server then receives
those bytes it will use them to generate the segment requested by the client.
After this is done the segment will be returned to the client. Another important
difference between this setup and the \gls{cdn} setup is that there is no
caching involved.  For a schematic overview of this setup see
figure~\vref{fig:ismproxy}.

\begin{figure}[h]
    \includegraphics{ismproxy_diagram.pdf}
    \caption{The IsmProxyPass setup}\label{fig:ismproxy}
\end{figure}

This setup shares some of the advantages of the \gls{cdn} setup. This setup can easily
be used to mitigate the storage overloading issue. By just adding more proxy
servers the CPU intensive conversion work can be shared by multiple servers. All
that the storage server has to do is serve the byte ranges that the proxy
servers need, which is not nearly as intensive as the conversion. The latency
issue however, is still present. Since for every clients request still needs to
reach the location of the storage server, it doesn't matter that they are
converted to range requests, they still need reach the storage eventually.

The issue with internal requests is again present and it is much worse then with
the \gls{cdn} setup. The first reason for this is that no caching takes place,
so every clients request has to travel further, instead of just the first one
for each segment. Another issue is that the request is that every request of a
client spawns multiple range requests, which means even more waiting.

So it seems that this setup is only worse than the CDN setup. However, it does
have one important advantage. Hosting large files on dedicated servers or VPS’s
can be quite expensive. This is why there are services that specialize just in
hosting files, like Amazon S3. Services like this, which will be called dumb
storage from now on, are quite popular for hosting large files, like videos,
because it is relatively cheap. Dumb storage allows files to be uploaded and it
will make them accessible over HTTP by supplying a URL\@. However, there is now
way to access the server internals, like the HTTP server that is running on the
server. This is no problem for static content, however, on the fly conversion is
no static content. The \gls{usp} software needs to be installed into the HTTP
server itself, which means it is impossible to use on dumb storage. This setup
solves this issue by separating the storage and the conversion step, which makes
it possible to host the mp4 files on dumb storage and use other servers for the
conversion.

\section{Improvement}
Both the \gls{cdn} setup and the \gls{ipp} setup have their clear advatages.
Both their disadvantages are clear as well. A combination between the \gls{cdn}
setup and the \gls{ipp} setup that would have both of their advantages could be
an ideal setup. However, currently no such setup exists. The clear advantage
would be that it is possible to combine the cheap dumb storage possiblities with
the speed of \gls{cdn} setup. However, another advantage is gained as well. The
proxy server would now be caching the range requests of the raw mp4 instead of
the request for a converted segment. This would mean that the same content
is not stored multiple times, like in the \gls{cdn} setup, since the same cached
range requests can be used to generate a segment in a different format.  This
would also mean that the amount of internal traffic would go down since no
requests would have to be done for the same segment in a different format. These
advantages are why this thesis sets out to design such a setup and to test its
performance against currently available setups.







\chapter{Design considerations}
% Explain the search for technologies used instead of stating the decision and
% then explaining why.
To build the setup proposed in the introduction, existing technology can used
heavily. This chapter will describe those technologies and explain how they can
be utilized.


\section{Webserver application on the proxy server}
Nginx is chosen as the webserver application for the reverse proxy in the
proposed setup. Apache could also be used, however Nginx has some features that
probably make it a better choice for the proposed setup.

\subsection{Caching}\label{sec:caching}
Since the proposed setup depends heavily on caching in the reverse proxy, it is
important that the server application does this wel. This is the case for Nginx.
Apache also does a good job at it, but one key difference is that cache locking is
supported better in Nginx. Cache locking means that the same content will not be
requested multiple times from the upstream server when multiple requests ask for
the same content at the same time and there is no cache entry for that
request. Instead only one request will be proxied through to the storage server
and the others wait until the response for that one is cached. This can
significantly reduce the load on the storage server.

\subsection{Scripting}
% In server lua vs CGI
Another reason for choosing Nginx are its extensive scripting capabilities. It
is possible to enable Lua scripting in the server itself by installing a special
module\autocite{nginxlua}. This can be very usefull if advanced logic is needed
to create the proposed setup.


\section{Caching range requests}
% Maybe use subsections so it will be less of a wall of text.
Since IsmProxyPass uses range requests it is also important that those can be
cached. This is not directly supported in any webserver. The reason for
this is that it is not exactly trivial to do. Most caches work by supplying it
some data to cache, together with a key by which the data can be retrieved later
on.
Ideally one would cache the content using the key of the full file and add some
info about what range is actually cached. Then when another request for a range
comes in it would check in the cache if some of the requested bytes are in
there. If not all of them are there it should request just the bytes that are
not cached from the upstream server and then add those to the same cache for
future use. This hasn't been implemented anywhere yet. However, our proposed
setup is not the first that would benefit from caching range requests. So some
suboptimal solutions have been proposed for different use cases.

The most simple one, which is employed by Nginx by default,
is removing the range request header from the request when sending it to
upstream. This will request the whole file instead of just the requesting byte
range. Nginx will then cache this and return the whole file everytime a range is
requested. According to the RFC this behaviour is allowed, but this it is
clearly not behaviour that one would want, since the whole point of range
requests is that requesting the full file is not necessary.

A simple solution proposed on the Nginx forums\autocite{nginxcacheforum} is to
force Nginx to request the range from upstream and then adding the requested
range to the cache key. This basically means that a range request is handled by
the cache as if it were a seperate URL\@. The main downside of this solution is that
when overlapping byte ranges are requested they will both be cached in full.
This means this  solution works fine when the ranges that get requested are
always the same. However if the requested ranges are different almost every
time this solution is not acceptable since a lot of the cache wil contain the
same bytes, because of the overlap in the byte ranges.

% Check wat proxy_force_ranges doet en of het te gebruiken is in mijn versie van

Ranger\autocite{ranger} is another solution to the problem that tries to solve
the overlap problem. Everytime a range request comes in, instead of sending it
directly to the upstream server, it generates other range requests that together
contain all the bytes requested. These new range requests are always from the
same block size and they also start at the same byte position. These requests
will then be sent to upstream and after a response comes back they will be
cached as well.  This way, even though the initial range request can be any
range and can overlap with others, the requests sent to upstream will always
match previously sent requests, which makes sure that cache hits occur every
time part of a range is requested that was requested before even though it's not
exactly the same. There are only two small issues with this approach. The first
one is that more data is requested from the upstream server than is actually
needed to fulfill the request. However, this is probably not too severe when the
block size is chosen small enough. The second issue is that when large ranges
are requested, a lot of requests need to be send to the upstream server, because
the large request gets split into a lot of smaller ones. Which could mean some
unnecessary overhead.





\chapter{Implementation}
The created server setup consists of two servers, in this case virtual machines.
One that has the video files on disk, this will be called the storage server,
and one that serves as a reverse proxy, this will be called the proxy server.
The proxy server can also be called the cache server in the cases where it
caches proxied requests.

The client requests a specific segment of the video in a specific format, say
\gls{iss}. The storage server only contains the mp4 file of the video. At some point
in the server chain the segment needs to be generated from (part of) that mp4.



\section{The Late Transmuxing configuration}
The last configuration is the one this thesis is about. The late transmuxing
configuration. This configuration is a mix of the previous two configurations.
Stating it very simply, it is a version of the IsmProxyPass configuration with
caching built in. This is accomplished by running two different server
applications on the proxy server. A request of the client is initially handled
on the proxy server by Nginx. Nginx then passes the request along to Apache,
running on the same server. Apache then uses IsmProxyPass from the Unified
Streaming module to send the range requests mentioned in the previous
configuration.  However, instead of sending those requests to the storage server
directly, it sends them back to Nginx. Nginx then passes those requests along to
the storage server. The storage server will again respond with the byte ranges
requested.

When Nginx receives a response to one of its range requests it should to cache
that response. This can be done quite easily by using the second method
described in section~\vref{sec:caching}, which was the method that meant
changing the cache key. The reason this method is sufficient and the third
method is not needed is because that method solves a problem that our use case
doesn't have.  It solves the problem of overlapping byte ranges. For video
streaming however, this problem doesn't exist because the because the fragments
that cen be requested are fixed. This means the same ranges will always be
requested and overlaps in those ranges should be minimal.

Apart from caching the response Nginx will also return it to Apache. Apache will
then generate the originally requested segment and return that back to Nginx.
Nginx will then cache the segment and return it to the client. See figure
\vref{fig:latetrans} for a schematic overview of this setup.

\begin{figure}
    \includegraphics{latetrans_diagram.pdf}
    \caption{The Late Transmuxing configuration}\label{fig:latetrans}
\end{figure}

\subsection{Using two webserver applications on the same server}
% Explain deadlocks


\section{Configurations with less caching}
Apart from these three main configurations, there are also configurations
with caching disabled at certain points in the server chains. These
configurations are created to find out what effect of each cache is on the
different performance metrics.






\chapter{Experiments}
\section{Setup}
\subsection{Comcast}
\subsection{wrk}
\section{Results}

\chapter{Discussion}
\section{Future Work}
\chapter{Conclusions}


\printbibliography[heading=bibintoc]{}

\printglossaries{}


\end{document}
