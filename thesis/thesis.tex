\documentclass[twoside,openright]{uva-bachelor-thesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}

%\usepackage[dutch]{babel}  % uncomment if you write in dutch
\usepackage{graphicx}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[style=ieee]{biblatex}
\addbibresource{thesis.bib}
\usepackage{lmodern}
%Enable for final compilation
%\usepackage[stretch=10]{microtype}
\usepackage{graphicx}
\usepackage{varioref} % Fancy references
\usepackage{caption} % Fixes hyperref to figure
\usepackage[toc,acronym,xindy]{glossaries}

\makeglossaries{}


\newacronym{abs}{ABS}{adaptive bitrate streaming}
\newacronym{iss}{ISS}{ISS Smooth Streaming}
\newacronym{dash}{DASH}{Dynamic Adaptive Streaming over HTTP}
\newacronym{hds}{HDS}{HTTP Dynamic Streaming}
\newacronym{hls}{HLS}{HTTP Live Streaming}
\newacronym{usp}{USP}{Unified Streaming}
\newacronym{cdn}{CDN}{content delivery network}
\newacronym{ipp}{IPP}{IsmProxyPass}


% Title Page
\title{Late Transmuxing:\\Improving caching \\in video streaming}
\author{Jelte Fennema}
\supervisors{Dirk Griffioen (Unified Streaming), Robert Belleman (UvA)}
\signedby{Robert Belleman (UvA)}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}


\tableofcontents

\chapter{Introduction}
% Start with the grand scheme of things with respect to VOD
The internet started out as a means of sending messages from one place to
another. Since then it has become much faster and is used by many different
applications. It is still used to send messages, but because storage became
cheaper and the internet became faster, it is now used for applications that
require way more bandwith. Because of it a revolution in the way we watch videos
has taken place. Movie theaters and cable TV are not the only ways to watch
movies and TV programs anymore. Streaming of videos over the internet has grown
to an enormous industry that can be used to watch any kind of video imaginable.
Lots of amature videos are shared using YouTube, movies and TV shows can be
watched easily using Netflix and even live TV is being moved to the internet,
for instance by the BBC iPlayer.

This way of video streaming is not limited to normal computers. It is done on on
lots of different devices, like smartphones, tablets, set-top boxes,
mediacenters or Smart TVs. Especially on mobile devices the network conditions
can suddenly change drastically, for instance when changing the network
connection from WiFi to a mobile network. With initial streaming technologies
these changes caused buffering issues when the client was trying to get a high
quality video using a low bandwith network connection. This brought life to the
notion of \gls{abs}, which is a way of streaming video that changes the quality
of the video that is being watched, based on what the network connection can
handle \autocite{abs}.

There are a couple of streaming solutions that implement \gls{abs}, most of
those use HTTP as the underlying protocol \autocite{http}.  One of the reasons
for this is that HTTP is already in heavy and this way it is possible to
leverage the already existing infrastructure that is in place for other HTTP
communication. A disadvantage of HTTP however is that it is a protocol for
sending whole files and entire video files are typically quite large. This is
why the solutions stream little sections of the video instead of the whole file.
This means that the original video file should be cut in smaller files. They
also need to store some extra metadata in those files, so a player will actually
know how to play it. The solutions all have their own (slightly) different way
of adding this metadata.

% Maybe start with usp here and tell about the problem they solve.
When hosting video these different formats can cause quite a bit of trouble.
The reason for this is that the different formats are not supported equally by
the players used by different devices or browsers. Because of this, a good video
host should have its videos available in a lot of different formats to make sure
they can be viewed by every device and browser. The most obvious way of doing
this ofcourse, is generating the files in all the needed formats and then
setting up a file server that simply serves all the little files. This works but
takes up quite a bit of space since the same video is essentially stored
multiple times.

\Gls{usp} is a company that provides an easy to use solution for this problem.
They do this by supplying modules for a set of popular HTTP servers, Apache,
Nginx, IIS and Lighttpd. The way their module solves the problem of storing the
video multiple times is by only storing the mp4 and converting that on the fly
to the requested format. At the moment four different \gls{abs} formats are
supported by the \gls{usp} software. \Gls{iss}, which is developed by Microsoft and is
used in Silverlight based players~\autocite{iss}. \Gls{hds}, which is developed by
Adobe and is used in Flash based players \autocite{hds}.  \Gls{hls}, which is developed
by Apple and used on the iPhone, iPad and Apple TVs~\autocite{hls}. The last
format is \gls{dash}, which is developed as an ISO standard and is used in
\textsc{HTML5} video players~\autocite{dash}.


\section{Traditional setups}
% Maybe less details
The simplest way on the fly conversion can be used in a setup, is again a
single server, that converts the mp4 to the needed format for every request.
However, in large streaming setups this is not enough for two reasons. The first
one is that the storage server might be far away from the viewer, which means
more time is spent waiting before watching a video. The second one is that the
storage server can be easily overwhelmed with requests, because its just one
server.

Using a \gls{cdn} with reverse proxy caching nodes is a simple solution to these
problems, which can also used when hosting lots of content besides
videos~\autocites{cdns}{revproxy}. Those nodes forward the request to the
storage server and cache the response. If another request comes for the same
content a node will just respond with the response in its cache. This works
quite good, since a small percentage of the videos account for a large
percentage of the views.
% For live streaming this also works fantastic since it's not even a whole video
% that is downloaded a lot, it is only the last part of that video.

The big problem of this setup is that, just like with the very first setup, it
stores the converted files instead of the raw files, only now in the cache
instead of the storage server, which has two downsides in this case. Again, a
server will essentially contain the same content multiple times, only in this
case the cache server. The other downside, specific to this setup, is that the
cache will also request basically the same content from the storage. Those
requests mean more internal traffic and more waiting time for the viewer.

\section{Approach}
% Very short exlaination of the idea to solve the problem.
The problem described above is why this thesis will explore the possibility of
using a ``Late Transmuxing''. This setup would do the conversion of the video
directly on the caching nodes, instead of at the storage level. This means that
the cache server would request segments of the mp4 file from the storage server,
instead of the already converted files. This approach should solve both the
extra storage problem and the internal traffic problem, because only the segments
of mp4 have to be stored and requested.






\chapter{Problem analysis}
% Explain the normal setups in detail together with their shortcommings.
The introduction briefly described some of the server setups used for streaming
video. This chapter will explain in detail what kind of setups are currently
available and what their advantages and disadvantages are. The setups that will
be looked into are the ones that convert an mp4 on the fly to different
\gls{abs} formats. The reason for this is that those setups already have a clear
advantage over servers that simply store files in all formats, namely the space
required to store the videos. To explain how each of the setups work, each step
will be described, from the client requesting a segment of video in a specific
format, to the client receiving that segment.

The different server setups consist of one or two servers. One server, which is
present in every setup, stores the the mp4 files on disk, this will be called
the storage server. The more advanced setups also contain a second server, which
serves as a reverse proxy, this server will be called the proxy server. In the
setups where a proxy server is used the client connects to the proxy server and
the proxy server then requests content from the storage server that it needs
to fulfill the request from the client. When caching takes place on the proxy
server, it can also be called the cache server.

Lastly, on both of these physical servers will run an HTTP server to serve the
HTTP requests. For illustration purposes a clear distinction between a
physical server and the HTTP server that runs on that server needs to be made.
This will be done by describing the setups as if Apache is used on the storage
server and Nginx is used on the proxy server. Keep in mind however that this is
just for illustration purposes, in practice any of the HTTP servers supported by
\gls{usp} can be used on any of the physical servers.


\section{The single server setup}
This setup was already briefly explained in the introduction and it is the most
simple setup that will be covered. In this setup a client requests a segment
directly from the storage server. Apache will then locate the mp4 that contains
the requested segment. It will then read the parts of the file that are
necessary to create the segment in the requested format. From these parts, it
will then create the segment and send it back to the client. After the client
receives it, it can be played by the player. For a schematic overview of this
setup see figure~\vref{fig:singleserver}.

\begin{figure}[h]
    \includegraphics{singleserver_diagram.pdf}
    \caption{The single server setup}\label{fig:singleserver}
\end{figure}

The main advantage of this setup is that is very simple to set up. It is just
one server and all it has to do store files and run the conversion code. The
main disadvantage is also that it is just one server. This means that it can
easily be overwhelmed with requests when traffic is high, especially since it
not only has to serve files, but also run the conversion code. Another issue
that occures because it is just a single server is that the time to reach the
client can be quite long when the client is located far away from the server,
for instance on another continent.

%The last disadvantage is that it scales very
%badly, since it is impossible to just add extra servers without duplicating the
%storage of all the videos to those servers as well. All that can be done is
%upgrading the existing server and this can only be done until a certain limit.


\section{The CDN setup}
Another setup that was already briefly introduced is the \gls{cdn} setup. It is
a quite a simple extension of the single server setup. In this setup the proxy
server will receive the request for the segment from the client. It then passes
that exact reqest along to the storage server. The storage server will then
generate the segment from the mp4 and send the segment back to the proxy server.
The proxy server will then send the response back to the client, but it will
also cache the response. That way, when a client requests the same segment in
the same format the proxy server will be able to serve the segment directly from
its cache, instead of requesting it from the storage server. For a schematic
overview of this setup see figure~\vref{fig:cdn}.

\begin{figure}[h]
    \includegraphics{cdn_diagram.pdf}
    \caption{The \gls{cdn} setup}\label{fig:cdn}
\end{figure}

Using this setup solves the issues present in the single server
setup. The simple server setup was easy to overload with lots of requests.
Caching should help quite a lot in this regard, since there normally is a set of
videos that receives a lot of views and a set that receives (almost) none.
Because of the caching, the storage only needs to convert the mp4 for the
popular videos once for each segment and format, instead of for every request.
The latency issue is also mostly mittigated by the caching. By simply adding
more proxy servers, spread around the globe, only the first request for a
segment will have high latency, the next request for that same segment only
needs to travel to the proxy server.

One of the disadvantages of this setup is ofcourse that it is less trivial to
set up than the single server setup. It also has some other disadvantages.
Because it caches the converted segments, it is essentially storing the same
part of the video multiple times, only in another format, which is exactly the
problem that on the fly conversion was there to solve. This time it is only
doing this however for the most frequently watched videos, so it is less of an
issue, but it is still something that is best kept to a minimum.

Another small disadvantage is caused by the proxy requests this server uses. A
request that is not cached yet will actually take longer to return to the
client than it would with the single server setup. This is because of the
overhead of first traveling to the proxy server and then to the storage server.
This is almost always a longer (and never a shorter) route then the route
directly to the storage server. Because this longer route is taken only the
first time a specific segment is requested and the next times the route will be
shorter this is not such a big problem, but it still is something that should be
noted.


\section{The IsmProxyPass setup}
Another setup that the \gls{usp} software supports is the \gls{ipp} setup. Just
like the \gls{cdn} setup, this setup extends the single server setup. However it
does this in quite a different way. The key difference is that the \gls{usp}
software does not run on the storage server, but on the proxy server. This
time, when the proxy server receives a request from a client, instead of passing
it directly to the storage, it will send a series of HTTP range requests, which
are generated by the \gls{usp} software, to the storage
server~\autocite{rangerequests}. Range requests are a type of HTTP requests that
do not request the whole file that the server would return for a normal HTTP
request, it only requests a specific byte range of that file.  These kind of
requests are especially useful when only a small piece of a large file is
needed.

Upon receiving one of those range requests, the storage server will simply
return the requested bytes from the mp4. When the proxy server then receives
those bytes it will use them to generate the segment requested by the client.
After this is done the segment will be returned to the client. For a schematic
overview of this setup see figure~\vref{fig:ismproxy}.

\begin{figure}[h]
    \includegraphics{ismproxy_diagram.pdf}
    \caption{The IsmProxyPass setup}\label{fig:ismproxy}
\end{figure}

This setup shares some of the advantages of the \gls{cdn} setup. It can easily
be used to mitigate the storage overloading issue. By just adding more proxy
servers the CPU intensive conversion work can be shared by multiple servers. All
that the storage server has to do is serve the byte ranges that the proxy
servers need, which is not nearly as intensive as the conversion. The latency
issue however, is still present. Since for every request of a client still needs
to reach the location of the storage server, it does not matter that they are
converted to range requests, they still need reach the storage eventually.

The issue with internal requests is again present and it is much worse than with
the \gls{cdn} setup. The first reason for this is that no caching takes place,
so every clients request has to travel further, instead of just the first
request for each segment. Another issue is that every request of a client spawns
multiple range requests, which means even more overhead and thus more waiting.

Because of all this, it seems that this setup is only worse than the \gls{cdn}
setup. However, it does have one important advantage. Hosting large files on
dedicated servers or VPS’s can be quite expensive. This is why there are
services that specialize just in hosting files, like Amazon~S3. Services like
this, which will be called dumb storage from now on, are quite popular for
hosting large files, like videos, because it is relatively cheap. Dumb storage
allows files to be uploaded and it will make them accessible over HTTP by
supplying a URL\@.  However, there is now way to access the server internals,
like the HTTP server that is running on the server. This is no problem for
static content, however, on the fly conversion makes the videos dynamic content.
The \gls{usp} software needs to be installed into the HTTP server itself, which
means it is impossible to use on dumb storage. This setup solves this issue by
separating the storage and the conversion tasks, which makes it possible to host
the mp4 files on dumb storage and use other servers for the conversion.

\section{Combining the setups}
Both the \gls{cdn} setup and the \gls{ipp} setup have their clear advatages.
Both their disadvantages are clear as well. A combination between the \gls{cdn}
setup and the \gls{ipp} setup that would have both of their advantages could be
an ideal setup. However, currently no such setup exists. The clear advantage
of a combinition like this would be that it is possible to combine cheap dumb
storage with the speed of \gls{cdn} setup. However, another advantage is gained
as well. In this combination the proxy server would be caching the range
requests of the raw mp4 instead of the request for a converted segment. This
would mean that the same content is not stored multiple times in the cache, like
in the \gls{cdn} setup, since the same cached range requests can be used to
generate a segment in a different format. This would also mean that the amount
of internal traffic would go down since no requests would have to be done for
the same segment in a different format. These advantages are why this thesis
sets out to design such a setup and to test its performance against currently
available setups.







\chapter{Design considerations}
% Explain the search for technologies used instead of stating the decision and
% then explaining why.
To build the proposed setup, existing technology can be used heavily. This
chapter will describe the technologies used and why they have been chosen over
other similar technologies.


\section{HTTP servers}
The proposed setup consists of two different servers the storage server and the
proxy server. Both of these servers need to have an HTTP server to handle the
incomming requests. Both the Nginx and Apache are very capable and popular HTTP
servers. They are also both available for Linux and they are supported by the
\gls{usp} software. There are quite a bit of differences that and some of those
are of importance for the proposed setup.


\subsection{General differences}
There are a couple of general differences between Apache and Nginx that have
already been thoroughly explored in a multitude of
resources~\autocites{nginx1}{nginx2}{apachevsnginx}. Their core architecture is
very different. Apache spawns a new thread for each connection, while Nginx
dispatches the requests to already existing threads. This means that Nginx can
handle a lot of concurrent requests with very little resources. Their take on
dynamic content is also quite different. Apache uses modules that can be easily
loaded into Apache itself to handle dynamic content. Nginx just receives the
requests and sends them to other applications, sometimes even Apache, if content
needs to be dynamically generated. This is why Nginx is usually a good choice as
a reverse proxy.


\subsection{USP integration}
While USP supports both servers fully, because of the architecture of both
servers it is easier to create a working setup with Apache. Apache can load
modules for dynamic functionality easily, which is how the \gls{usp}
installation works~\autocite{uspapache}. Just download and install the module
and enable it by adding some lines to the server config files. Nginx however is
not designed to just load modules on the fly. For dynamic content it normally
just contacts an external application. However, it is possible to extend Nginx
by including a module at compile time, so that is how the \gls{usp} module
needs to be installed~\autocite{uspnginx}. This means it is not possible to just
use the Nginx supplied by the Linux distribution of choice. So it is quite clear
that the setup for Apache is quite a bit easier than the one for Nginx.


\subsection{Caching}
Since the proposed setup depends heavily on caching in the reverse proxy, it is
important that the server application does this wel. This is both the case for
Nginx and Apache. There is a key difference however, this is the cache locking
support better in Nginx. Cache locking means that the same content will not be
requested multiple times from the upstream server when multiple requests ask for
the same content at the same time and there is no cache entry for that request.
Instead only one request will be proxied through to the storage server and the
others wait until the response for that one is cached. This can significantly
reduce the load on the storage server. Both Nginx and Apache support cache
locking. However, on Apache the server only uses it as a hint and it will still
fetch from the storage if it receives a request with a header enable that tells
it to do so~\autocite{apachecache}. This is not the case for
Nginx~\autocite{nginxcache}.


\subsection{Scripting}
% In server lua vs CGI
Another reason for choosing Nginx are its extensive scripting capabilities. It
is possible to enable Lua scripting in the server itself by installing a special
module~\autocites{nginxlua}{cloudflarelua}. This can be very useful if advanced
logic is needed to create the proposed setup.

\subsection{Decision}
Nginx is chosen as the HTTP server for the reverse proxy in the
proposed setup. The scripting capabilities and better support for cache locking
are the main reasons, since those could be useful for the actual setup. Its
low memory footprint and speed are nice to have, but not essential for the setup
itself. Which type of HTTP server is used by the storage server is not really
important for this specific setup either, since they can both serve static files
and support range requests out of the box. For the setup that will be used for
testing the performance Apache will be used on the storage server, the reason
for this is that the storage server needs on the fly conversion support to be
able to simulate the \gls{cdn} setup and it is easier to enable the \gls{usp}
module in Apache than in Nginx.




\section{Caching range requests}\label{sec:caching}
% Maybe use subsections so it will be less of a wall of text.
Since IsmProxyPass uses range requests it is also important that those can be
cached. This is not directly supported in any webserver. The reason for
this is that it is not exactly trivial to do. Most caches work by supplying it
some data to cache, together with a key by which the data can be retrieved later
on.

Ideally one would cache the content using the key of the full file and add some
info about what range is actually cached. Then when another request for a range
comes in it would check in the cache if some of the requested bytes are in
there. If not all of them are there it should request just the bytes that are
not cached from the upstream server and then add those to the same cache for
future use. This hasn't been implemented anywhere yet. However, our proposed
setup is not the first that would benefit from caching range requests. So some
suboptimal solutions have been proposed for different use cases.

\subsection{Ignore range}
The most simple one, which is employed by Nginx by default,
is removing the range request header from the request when sending it to
upstream. This will request the whole file instead of just the requesting byte
range. Nginx will then cache this and return the whole file everytime a range is
requested. According to the RFC this behaviour is allowed, but this it is
clearly not behaviour that one would want, since the whole point of range
requests is that requesting the full file is not necessary.

\subsection{Match exact range}
A simple solution proposed on the Nginx forums\autocite{nginxcacheforum} is to
force Nginx to request the range from upstream and then adding the requested
range to the cache key. This basically means that a range request is handled by
the cache as if it were a seperate URL\@. The main downside of this solution is that
when overlapping byte ranges are requested they will both be cached in full.
This means this  solution works fine when the ranges that get requested are
always the same. However if the requested ranges are different almost every
time this solution is not acceptable since a lot of the cache wil contain the
same bytes, because of the overlap in the byte ranges.

% Check wat proxy_force_ranges doet en of het te gebruiken is in mijn versie van
\subsection{Ranger}
Ranger is another solution to the problem that tries to solve the overlap
problem~\autocite{ranger}. Everytime a range request comes in, instead of
sending it directly to the upstream server, it generates other range requests
that together contain all the bytes requested. These new range requests are
always from the same block size and they also start at the same byte position.
These requests will then be sent to upstream and after a response comes back
they will be cached as well.  This way, even though the initial range request
can be any range and can overlap with others, the requests sent to upstream will
always match previously sent requests, which makes sure that cache hits occur
every time part of a range is requested that was requested before even though
it's not exactly the same. There are only two small issues with this approach.
The first one is that more data is requested from the upstream server than is
actually needed to fulfill the request. However, this is probably not too severe
when the block size is chosen small enough. The second issue is that when large
ranges are requested, a lot of requests need to be send to the upstream server,
because the large request gets split into a lot of smaller ones. Which could
mean some unnecessary overhead.

\subsection{Decision}





\chapter{Implementation}
The created server setup consists of two servers, in this case virtual machines.
One that has the video files on disk, this will be called the storage server,
and one that serves as a reverse proxy, this will be called the proxy server.
The proxy server can also be called the cache server in the cases where it
caches proxied requests.

The client requests a specific segment of the video in a specific format, say
\gls{iss}. The storage server only contains the mp4 file of the video. At some point
in the server chain the segment needs to be generated from (part of) that mp4.



\section{The Late Transmuxing configuration}
The last configuration is the one this thesis is about. The late transmuxing
configuration. This configuration is a mix of the previous two configurations.
Stating it very simply, it is a version of the IsmProxyPass configuration with
caching built in. This is accomplished by running two different server
applications on the proxy server. A request of the client is initially handled
on the proxy server by Nginx. Nginx then passes the request along to Apache,
running on the same server. Apache then uses IsmProxyPass from the Unified
Streaming module to send the range requests mentioned in the previous
configuration.  However, instead of sending those requests to the storage server
directly, it sends them back to Nginx. Nginx then passes those requests along to
the storage server. The storage server will again respond with the byte ranges
requested.

When Nginx receives a response to one of its range requests it should to cache
that response. This can be done quite easily by using the second method
described in section~\vref{sec:caching}, which was the method that meant
changing the cache key. The reason this method is sufficient and the third
method is not needed is because that method solves a problem that our use case
doesn't have.  It solves the problem of overlapping byte ranges. For video
streaming however, this problem doesn't exist because the because the fragments
that cen be requested are fixed. This means the same ranges will always be
requested and overlaps in those ranges should be minimal.

Apart from caching the response Nginx will also return it to Apache. Apache will
then generate the originally requested segment and return that back to Nginx.
Nginx will then cache the segment and return it to the client. See figure
\vref{fig:latetrans} for a schematic overview of this setup.

\begin{figure}
    \includegraphics{latetrans_diagram.pdf}
    \caption{The Late Transmuxing configuration}\label{fig:latetrans}
\end{figure}

\subsection{Using two webserver applications on the same server}
% Explain deadlocks


\section{Configurations with less caching}
Apart from these three main configurations, there are also configurations
with caching disabled at certain points in the server chains. These
configurations are created to find out what effect of each cache is on the
different performance metrics.






\chapter{Experiments}
\section{Setup}
\subsection{The different setups}

\subsection{Virtual machines}
To create the proposed setup a storage server and a proxy server are needed. A
possibility is to use physical servers for this. This will however


\subsubsection{Comcast}
Since the VMs will be running on the same machine there are almost no network
limitations. That is why artificial network throtteling is needed. Comcast is a
tool that

\subsection{The wrk tool}
\subsection{The tests}

\section{Results}

\chapter{Discussion}
\section{Future Work}
\chapter{Conclusions}


\printbibliography[heading=bibintoc]{}

\printglossaries{}


\end{document}
