\documentclass[twoside,openright]{uva-bachelor-thesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}

%\usepackage[dutch]{babel}  % uncomment if you write in dutch
\usepackage{graphicx}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[style=ieee]{biblatex}
\addbibresource{thesis.bib}
\usepackage{lmodern}
%Enable for final compilation
%\usepackage[stretch=10]{microtype}
\usepackage{graphicx}
\usepackage{varioref} % Fancy references
\usepackage{caption} % Fixes hyperref to figure
\usepackage[toc,acronym,xindy]{glossaries}
\usepackage{xspace}
\usepackage{nameref}
\usepackage{cleveref}
\usepackage[binary-units]{siunitx}
\usepackage{placeins}
\usepackage{float}

\makeglossaries{}

\newcommand{\betternewacronym}[3]{\newacronym{#1}{#2}{#3}
\expandafter\newcommand\csname #1\endcsname{\gls{#1}\xspace}
\expandafter\newcommand\csname #1up\endcsname{\Gls{#1}\xspace}
\expandafter\newcommand\csname #1long\endcsname{\acrlong{#1}\xspace}
}


\betternewacronym{abs}{ABS}{adaptive bitrate streaming}
\betternewacronym{iss}{ISS}{ISS Smooth Streaming}
\betternewacronym{dash}{DASH}{Dynamic Adaptive Streaming over HTTP}
\betternewacronym{hds}{HDS}{HTTP Dynamic Streaming}
\betternewacronym{hls}{HLS}{HTTP Live Streaming}
\betternewacronym{usp}{USP}{Unified Streaming}
\betternewacronym{cdn}{CDN}{content delivery network}
\betternewacronym{ipp}{IPP}{IsmProxyPass}
\betternewacronym{lt}{LT}{late transmuxing}

\newcommand{\mbps}[1]{\SI[per-mode=symbol]{#1}{\mega\bit\per\second}}
\newcommand{\Mbps}[1]{\SI[per-mode=symbol]{#1}{\mega\byte\per\second}}


% Title Page
\title{Late Transmuxing:\\Improving caching \\in video streaming}
\author{Jelte Fennema}
\supervisors{Dirk Griffioen (Unified Streaming), Robert Belleman (UvA)}
\signedby{Robert Belleman (UvA)}


\begin{document}
\maketitle
% \pagenumbering{gobble}

\begin{abstract}
    With the growth of the internet, usage has shifted from sending textual
    messages to using it to stream video. Because of the different devices and
    players used to watch video streams, multiple different streaming formats
    are used to view the same video. Many known server setups already exist,
    trying to serve these different formats fast and resource efficient. Most
    use some form of proxy servers to reduce the load on the storage backend.
    Some use these proxy servers for caching and some use them for on the fly
    conversion. None of the setups however, utilise the power of caching and on
    the fly conversion on the same server. Creating a setup that combines both,
    leads to a setup with before unseen capabilities. Videos can be served
    directly from the proxy server in formats that have never been requests from
    the server before. Compared to other setups this lowers internal traffic
    and reduces load on the storage server. All of which results in faster,
    cheaper and more efficient video streaming.

\end{abstract}


\tableofcontents

\chapter{Introduction}
% Start with the grand scheme of things with respect to VOD
The internet started out as a means of sending messages from one place to
another. Since then it has become much faster and is used by many different
applications. It is still used to send messages, but because storage became
cheaper and the internet became faster, it is now used for applications that
require way more bandwidth. Because of it a revolution in the way we watch videos
has taken place. Movie theaters and cable TV are not the only ways to watch
movies and TV programs anymore. Streaming of videos over the internet has grown
to an enormous industry that can be used to watch any kind of video imaginable.
Lots of amateur videos are shared using YouTube, movies and TV shows can be
watched easily using Netflix and even live TV is being moved to the internet,
for instance by the BBC iPlayer.

This way of video streaming is not limited to normal computers. It is done on on
lots of different devices, like smartphones, tablets, set-top boxes,
media centers or Smart TVs. Especially on mobile devices the network conditions
can suddenly change drastically, for instance when changing the network
connection from WiFi to a mobile network. With initial streaming technologies
these changes caused buffering issues when the client was trying to get a high
quality video using a low bandwidth network connection. This brought life to the
notion of \abs, which is a way of streaming video that changes the quality
of the video that is being watched, based on what the network connection can
handle \autocite{abs}.

There are a couple of streaming solutions that implement \abs, most of
those use HTTP as the underlying protocol \autocite{http}.  One of the reasons
for this is that HTTP is already used widely so it becomes possible to
leverage the already existing infrastructure that is in place for other HTTP
communication. A disadvantage of HTTP however is that it is a protocol for
sending whole files and entire video files are typically quite large. This is
why the solutions stream little sections of the video instead of the whole file.
This means that the original video file should be cut in smaller files. They
also need to store some extra data in those files, so a player will actually
know how to play it. The solutions all have their own different way of adding
this data.

% Maybe start with usp here and tell about the problem they solve.
When hosting video these different formats can cause quite a bit of trouble.
The reason for this is that the different formats are not supported equally by
the players used by different devices or browsers. Because of this, a good video
host should have its videos available in a lot of different formats to make sure
they can be viewed by every device and browser. The most obvious way of doing
this of course, is generating the files in all the needed formats and then
setting up a file server that simply serves all the little files. This works but
takes up quite a bit of space since the same video is essentially stored
multiple times.

\Gls{usp} is a company that provides an easy to use solution for this problem.
They do this by supplying modules for a set of popular HTTP servers, Apache,
Nginx, IIS and Lighttpd. The way their module solves the problem of storing the
video multiple times is by only storing the mp4 and converting that on the fly
to the requested format. At the moment four different \gls{abs} formats are
supported by the \gls{usp} software. \Gls{iss}, which is developed by Microsoft and is
used in Silverlight based players~\autocite{iss}. \Gls{hds}, which is developed by
Adobe and is used in Flash based players \autocite{hds}.  \Gls{hls}, which is developed
by Apple and used on the iPhone, iPad and Apple TVs~\autocite{hls}. The last
format is \gls{dash}, which is developed as an ISO standard and is used in
\textsc{HTML5} video players~\autocite{dash}.


\section{Traditional setups}
% Maybe less details
The simplest way on the fly conversion can be used in a setup, is again a
single server, that converts the mp4 to the needed format for every request.
However, in large streaming setups this is not enough for two reasons. The first
one is that the storage server might be far away from the viewer, which means
more time is spent waiting before watching a video. The second one is that the
storage server can be easily overwhelmed with requests, because its just one
server.

Using a \gls{cdn} with reverse proxy caching nodes is a simple solution to these
problems, which can also used when hosting lots of content besides
videos~\autocites{cdns}{revproxy}. Those nodes forward the request to the
storage server and cache the response. If another request comes for the same
content a node will just respond with the response in its cache. This works
quite good, since a small percentage of the videos account for a large
percentage of the views.
% For live streaming this also works fantastic since it's not even a whole video
% that is downloaded a lot, it is only the last part of that video.

The big problem of this setup is that, just like with the very first setup, it
stores the converted files instead of the raw files, only now in the cache
instead of the storage server, which has two downsides in this case. Again, a
server will essentially contain the same content multiple times, only in this
case the cache server. The other downside, specific to this setup, is that the
cache will also request basically the same content from the storage. Those
requests mean more internal traffic and more waiting time for the viewer.

\section{Approach}
% Very short exlaination of the idea to solve the problem.
The problem described above is why this thesis will explore the possibility of
using a ``Late Transmuxing''. This setup would do the conversion of the video
directly on the caching nodes, instead of at the storage level. This means that
the cache server would request segments of the mp4 file from the storage server,
instead of the already converted files. This approach should solve both the
extra storage problem and the internal traffic problem, because only the segments
of mp4 have to be stored and requested.






\chapter{Problem analysis}\label{ch:problem}
% Explain the normal setups in detail together with their shortcommings.
The introduction briefly described some of the server setups used for streaming
video. This chapter will explain in detail what kind of setups are currently
available and what their advantages and disadvantages are. The setups that will
be looked into are the ones that convert an mp4 on the fly to different
\gls{abs} formats. The reason for this is that those setups already have a clear
advantage over servers that simply store files in all formats, namely the space
required to store the videos. To explain how each of the setups work, each step
will be described, from the client requesting a segment of video in a specific
format, to the client receiving that segment.

The different server setups consist of one or two servers. One server, which is
present in every setup, stores the mp4 files on disk, this will be called
the storage server. The more advanced setups also contain a second server, which
serves as a reverse proxy, this server will be called the proxy server. In the
setups where a proxy server is used the client connects to the proxy server and
the proxy server then requests content from the storage server that it needs
to fulfill the request from the client. When caching takes place on the proxy
server, it can also be called the cache server.

Lastly, on both of these physical servers will run an HTTP server to serve the
HTTP requests. For illustration purposes a clear distinction between a
physical server and the HTTP server that runs on that server needs to be made.
This will be done by describing the setups as if Apache is used on the storage
server and Nginx is used on the proxy server. Keep in mind however that this is
just for illustration purposes, in practice any of the HTTP servers supported by
\gls{usp} can be used on any of the physical servers.


\section{The single server setup}
This setup was already briefly explained in the introduction and it is the most
simple setup that will be covered. In this setup a client requests a segment
directly from the storage server. Apache will then locate the mp4 that contains
the requested segment. It will then read the parts of the file that are
necessary to create the segment in the requested format. From these parts, it
will then create the segment and send it back to the client. After the client
receives it, it can be played by the player. For a schematic overview of this
setup see figure~\vref{fig:singleserver}.

\begin{figure}[h]
    \includegraphics{singleserver_diagram.pdf}
    \caption{The single server setup}\label{fig:singleserver}
\end{figure}

The main advantage of this setup is that is very simple to set up. It is just
one server and all it has to do store files and run the conversion code. The
main disadvantage is also that it is just one server. This means that it can
easily be overwhelmed with requests when traffic is high, especially since it
not only has to serve files, but also run the conversion code. Another issue
that occurs because it is just a single server is that the time to reach the
client can be quite long when the client is located far away from the server,
for instance on another continent.

%The last disadvantage is that it scales very
%badly, since it is impossible to just add extra servers without duplicating the
%storage of all the videos to those servers as well. All that can be done is
%upgrading the existing server and this can only be done until a certain limit.


\section{The CDN setup}
Another setup that was already briefly introduced is the \gls{cdn} setup. It is
a quite a simple extension of the single server setup. In this setup the proxy
server will receive the request for the segment from the client. It then passes
that exact request along to the storage server. The storage server will then
generate the segment from the mp4 and send the segment back to the proxy server.
The proxy server will then send the response back to the client, but it will
also cache the response. That way, when a client requests the same segment in
the same format the proxy server will be able to serve the segment directly from
its cache, instead of requesting it from the storage server. For a schematic
overview of this setup see figure~\vref{fig:cdn}.

\begin{figure}[h]
    \includegraphics{cdn_diagram.pdf}
    \caption{The \gls{cdn} setup}\label{fig:cdn}
\end{figure}

Using this setup solves the issues present in the single server
setup. The simple server setup was easy to overload with lots of requests.
Caching should help quite a lot in this regard, since there normally is a set of
videos that receives a lot of views and a set that receives (almost) none.
Because of the caching, the storage only needs to convert the mp4 for the
popular videos once for each segment and format, instead of for every request.
The latency issue is also mostly mitigated by the caching. By simply adding
more proxy servers, spread around the globe, only the first request for a
segment will have high latency, the next request for that same segment only
needs to travel to the proxy server.

One of the disadvantages of this setup is of course that it is less trivial to
set up than the single server setup. It also has some other disadvantages.
Because it caches the converted segments, it is essentially storing the same
part of the video multiple times, only in another format, which is exactly the
problem that on the fly conversion was there to solve. This time it is only
doing this however for the most frequently watched videos, so it is less of an
issue, but it is still something that is best kept to a minimum.

Another small disadvantage is caused by the proxy requests this server uses. A
request that is not cached yet will actually take longer to return to the
client than it would with the single server setup. This is because of the
overhead of first traveling to the proxy server and then to the storage server.
This is almost always a longer (and never a shorter) route then the route
directly to the storage server. Because this longer route is taken only the
first time a specific segment is requested and the next times the route will be
shorter this is not such a big problem, but it still is something that should be
noted.


\section{The IsmProxyPass setup}
Another setup that the \gls{usp} software supports is the \gls{ipp} setup. Just
like the \gls{cdn} setup, this setup extends the single server setup. However it
does this in quite a different way. The key difference is that the \gls{usp}
software does not run on the storage server, but on the proxy server. This
time, when the proxy server receives a request from a client, instead of passing
it directly to the storage, it will send a series of HTTP range requests, which
are generated by the \gls{usp} software, to the storage
server~\autocite{rangerequests}. Range requests are a type of HTTP requests that
do not request the whole file that the server would return for a normal HTTP
request, it only requests a specific byte range of that file.  These kind of
requests are especially useful when only a small piece of a large file is
needed.

\begin{figure}[h]
    \includegraphics{ismproxy_diagram.pdf}
    \caption{The IsmProxyPass setup}\label{fig:ismproxy}
\end{figure}

Upon receiving one of those range requests, the storage server will simply
return the requested bytes from the mp4. When the proxy server then receives
those bytes it will use them to generate the segment requested by the client.
After this is done the segment will be returned to the client. This all works by
using \glspl{usp} custom \ipplong server directive and supply it a URL at
which it can reach the storage server~\autocites{uspismproxy}. For a schematic overview of this setup
see figure~\vref{fig:ismproxy}.

This setup shares some of the advantages of the \gls{cdn} setup. It can easily
be used to mitigate the storage overloading issue. By just adding more proxy
servers the CPU intensive conversion work can be shared by multiple servers. All
that the storage server has to do is serve the byte ranges that the proxy
servers need, which is not nearly as intensive as the conversion. The latency
issue however, is still present. Since for every request of a client still needs
to reach the location of the storage server, it does not matter that they are
converted to range requests, they still need reach the storage eventually.

The issue with internal requests is again present and it is much worse than with
the \gls{cdn} setup. The first reason for this is that no caching takes place,
so every clients request has to travel further, instead of just the first
request for each segment. Another issue is that every request of a client spawns
multiple range requests, which means even more overhead and thus more waiting.

Because of all this, it seems that this setup is only worse than the \gls{cdn}
setup. However, it does have one important advantage. Hosting large files on
dedicated servers or VPSs can be quite expensive. This is why there are
services that specialize just in hosting files, like Amazon~S3. Services like
this, which will be called dumb storage from now on, are quite popular for
hosting large files, like videos, because it is relatively cheap. Dumb storage
allows files to be uploaded and it will make them accessible over HTTP by
supplying a URL\@.  However, there is now way to access the server internals,
like the HTTP server that is running on the server. This is no problem for
static content, however, on the fly conversion makes the videos dynamic content.
The \gls{usp} software needs to be installed into the HTTP server itself, which
means it is impossible to use on dumb storage. This setup solves this issue by
separating the storage and the conversion tasks, which makes it possible to host
the mp4 files on dumb storage and use other servers for the conversion.

\section{Combining the setups}
Both the \gls{cdn} setup and the \gls{ipp} setup have their clear advantages.
Both their disadvantages are clear as well. A combination between the \gls{cdn}
setup and the \gls{ipp} setup that would have both of their advantages could be
an ideal setup. However, currently no such setup exists. The clear advantage
of a combination like this would be that it is possible to combine cheap dumb
storage with the speed of \gls{cdn} setup. However, another advantage is gained
as well. In this combination the proxy server would be caching the range
requests of the raw mp4 instead of the request for a converted segment. This
would mean that the same content is not stored multiple times in the cache, like
in the \gls{cdn} setup, since the same cached range requests can be used to
generate a segment in a different format. This would also mean that the amount
of internal traffic would go down since no requests would have to be done for
the same segment in a different format. These advantages are why this thesis
sets out to design such a setup and to test its performance against currently
available setups.







\chapter{Design considerations}\label{ch:design}
% Explain the search for technologies used instead of stating the decision and
% then explaining why.
To build the proposed setup, some design decisions have be made. Most of those
decisions concern the technologies to be used, but there are some other ones as
well. This chapter will describe the options to choose from. It will also
explain what the final decision for those choices is and why.


\section{HTTP servers}
The proposed setup consists of two different servers, the storage server and the
proxy server. Both of these servers need to have an HTTP server to handle the
incoming requests. Nginx and Apache are both very capable and popular HTTP
servers. They are also both available for Linux and they are supported by the
\gls{usp} software. However, there are quite a different in some aspects and some
of those aspects are important for the proposed setup.


\subsection{General differences}
There are a couple of general differences between Apache and Nginx that have
already been thoroughly explored in a multitude of
resources~\autocites{nginx1}{nginx2}{apachevsnginx}{nginxperformancescale}.
Their core architecture is very different. Apache spawns a new thread for each
connection, while Nginx dispatches the requests to already existing threads.
This means that Nginx can handle a lot more concurrent requests than Apache with
the same resources, since spawning threads is quite expensive resource wise.
Their take on dynamic content is also quite different. Apache uses modules that
can be easily loaded into Apache itself to handle dynamic content.  Nginx just
receives the requests and sends them to other applications if content needs to
be dynamically generated, in some setups Apache is even one of those other
applications. These differences are why Apache shines as an application server,
but Nginx is usually a good choice for serving as a static file server or as a
reverse proxy.


\subsection{USP integration}
While USP supports both servers fully, because of the architecture of both
servers it is easier to create a working setup with Apache. Apache can load
modules for dynamic functionality easily, so this is used by the \gls{usp}
software~\autocite{uspapache}. Just download and install the module
and enable it by adding some lines to the server configuration files. Nginx however is
not designed to just load modules on the fly. For dynamic content it normally
just contacts an external application. However, it is possible to add new
functionality to Nginx itself by including a module at compile time. This is
what needs to be done to install the \gls{usp} software for Nginx. This means
it is not possible to just use the Nginx supplied by the Linux distribution of
choice. This all makes it quite clear that the installation process for Apache
is quite a bit easier than the one for Nginx.


\subsection{Caching}
Since the proposed setup depends heavily on caching in the reverse proxy, it is
important that the server application does this well. Both Nginx and Apache have
good caching support~\autocites{nginxcache}{apachecache}. There is a key
difference however, this is the better cache locking support in Nginx. Cache
locking is a mechanism that can be used to reduce load on the storage server.
Normally if a request comes in for some content and there is no cache for it,
it will be proxied through to the storage server. This happens even if another
request for the same content is already being proxied through but has not
returned yet. If cache locking is enabled, the second request for some content
will wait until the first request has returned and fills the cache. This can
significantly reduce the load on the storage server, since the second request
can just fetch the response from the cache. Nginx and Apache both support cache
locking. However, Apache only uses it as a hint and it will still fetch from the
storage if it receives a request with a header enable that tells it to do so.
Nginx will always make a second request wait if cache locking is enabled, even
with special headers.


\subsection{Scripting}
% In server lua vs CGI
Both servers allow for low level scripting using
Lua~\autocites{nginxlua}{cloudflarelua}{apachelua}. This can be very useful if
complicated logic is needed to handle a request correctly. One big difference
between the Apache version and the Nginx version is that the Nginx version
can spawn subrequests, which can be very useful. For instance, one of the things
subrequests allow, is that based on some checks in the Lua code the request can
be handled by two entirely different Nginx paths. This way one can use the power
of the complicated lua logic in combination with the simpleness of normal
configurations.


\subsection{The decision}
Nginx is chosen as the HTTP server for the reverse proxy in the proposed setup.
The better support for cache locking and scripting are the main reasons, since
those could be useful for the actual setup. Its low memory footprint and speed
are nice side effects, but they are not essential for the setup itself.  Which
type of HTTP server is used by the storage server is not really important for
this specific setup, since they can both serve static files and support
range requests out of the box. Apache will be chosen for the setup that will be
tested, not because it is better for this setup, but because it is easier to
enable the \gls{usp} software for it. This will make it easier to test against
other setups that need on the fly conversion on the storage server, like the
\gls{cdn} setup.




\section{What responses to cache}
The proposed setup is basically a version the \gls{ipp} setup with
caching enabled. Because of this, it is important to decide what it is that
needs to be cached exactly. There are two types of responses that could be
cached on the proxy server, the range request responses it receives from the
storage server and the converted segments it sends to the client. The
range requests responses are most important to cache. These are most important
since they cause network traffic between the proxy server and the storage
server, which introduces latency.

The responses the proxy server sends to the client might also be useful to
cache. In the \gls{cdn} setup these responses were mostly cached because of
network latency. However, when already caching the range requests in the
\gls{ipp} setup, network latency shouldn't be an issue since the responses can
be generated on the fly from the cached range requests. This generating however
is CPU intensive, so caching the result of this conversion should reduce the CPU
load. Caching the converted segments still has the storage disadvantage it had
with the \gls{cdn} setup. The same segments are essentially cached multiple
times and this is even worse when also caching the range requests, since that
means segments are cached one extra time in comparison to the \gls{cdn} setup.


\subsection{The decision}
Currently it is unclear what the effect of caching each type of response has on
the performance of the setup. This is why the final implementation will come in two
flavors. A setup where all of the previously described responses are cached and
one where only the range requests are cached. A third flavor would be one where
only the responses to the client are cached, but since this flavor would
basically be the same as the \cdn setup it is not necessary when also testing
against that setup.




\section{Caching range requests}\label{sec:cachingrange}
% Maybe use subsections so it will be less of a wall of text.
Since the \ipp setup uses range requests it is also important that those
can be cached. This is not directly supported in Nginx or Apache. The reason for
this is that it is not exactly trivial to do. Most caches work by supplying it
with some data to cache, together with a key by which the data can be retrieved later
on.


Ideally the content would be cached using the key of the full file and some info
would be saved about what range is actually cached. Then when another request
for a range comes in it would check in the cache if some of the requested bytes
are in there. If not all of them are there it should request just the bytes that
are not cached from the upstream server and then add those to the same cache
entry for future use. A cache that works like this hasn't been implemented by
Apache or Nginx and quite possible by no HTTP server in existence. However, our
proposed setup is not the first that would benefit from caching range requests.
So some suboptimal solutions have been proposed for different use cases.

\subsection{Ignore range}
The most simple one, which is employed by Nginx by default, is removing the
range request header from the request when sending it upstream. This will
request the whole file instead of just the requesting byte range. Nginx will
then cache this and return the whole file every time a range is requested.
According to the RFC this behaviour is allowed, but this it is clearly not
desired behaviour, since the whole point of range requests is that requesting
the full file is not necessary.

\subsection{Match exact range}
A simple solution to this, proposed on the Nginx forums, is to force Nginx to
request the range from upstream and then adding the requested range to the cache
key~\autocite{nginxcacheforum}. This basically means that a range request is
handled by the cache as if it were a separate URL\@. The main downside of this
solution is that when overlapping byte ranges are requested they will both be
cached in full.  This means this  solution works fine when the ranges that get
requested are always the same. However if the requested ranges are different
almost every time, this solution is not acceptable since a lot of the cache will
contain the same bytes, because of the overlap in the byte ranges.

% Check wat proxy_force_ranges doet en of het te gebruiken is in mijn versie van
\subsection{Ranger}
Ranger extends further on the previous solution and tries to solve the overlap
problem~\autocite{ranger}. Every time a range request comes in, instead of
sending it directly to the upstream server, it generates other range requests
that together contain all the bytes requested. These new range requests always
have the same block size and they also start at the same byte position.  These
requests will then be sent to upstream and after a response comes back they will
be cached instead of the original request.  This way, even though the initial
range request can be any range and can overlap with others, the requests sent to
upstream will always match previously sent requests, which makes sure that cache
hits occur every time part of a range is requested that was requested before,
even though it's not exactly the same. There are only two small issues with this
approach.  The first one is that more data is requested from the upstream server
than is actually needed to fulfill the request. However, this is probably not
too severe when the block size is chosen small enough. The second issue is that
when large ranges are requested, a lot of requests need to be send to the
upstream server, because the large request gets split into a lot of smaller
ones. Which could mean some unnecessary overhead.

\subsection{The decision}
At this point it is clear that the first approach for caching range requests is
not sufficient, since that would mean the whole video would have to be sent to
the proxy server to send a small segment of that video to the client. The second
approach is one that shows great promise for the proposed setup. If the timing
and length of segments sent for each format are the same, it would probably mean
that the ranges requested are exactly the same as well. This would mean that the
caching with this approach would just work. However, if after preliminary
testing it is shown that a lot of the ranges are (slightly) different, using
ranger might be preferable over the second approach.











\chapter{Implementation}
The implementation of the \gls{lt} setup consists of configuration files for the
HTTP servers running on the proxy server and the storage server. What it
implements, stating it very simply, is a version of the \gls{ipp} setup with
caching enabled. This chapter will explain at a high level how the
implementation works. The implementation described is the flavor that caches all
types of responses, not just the range requests. The reason for this is that the
other flavor can simply be achieved by disabling the caching of the responses to
the client.


\section{Initial implementation}\label{sec:initimplement}
With everything from \nameref{ch:design} in mind the initial \lt setup was
implemented in the way described here. The setup should consist of two servers,
a proxy server that runs the \usp code and a storage server that serves byte
ranges. The proxy server should be able to cache the responses it sends to the
client and the responses it receives from the storage server. The \usp code does
not support caching natively. This can easily be solved by letting Nginx make
requests to itself. It can then use its normal proxy and caching mechanisms to
pass requests on to the actual destination and cache the responses it gets
before returning those to the original requester.

A schematic overview of the setup described next can be found in figure
\vref{fig:latetransold}. First, client requests a segment from the proxy server.
Nginx receives this request and passes it through its caching proxy to the
\ipplong handler. This handler is setup to request the byte ranges from a second
proxy running in Nginx. It then uses the \usp software to generate those range
requests and requests them from that proxy. The second proxy then passes those
requests on to the storage server. The storage server will then receive the
requests and return the requested byte ranges.

When the second proxy in Nginx receives a response to one of its range requests
it caches that response. This is done by using the second method described in
section~\ref{sec:cachingrange}. Apart from caching the response the proxy will
send it back to the \ipplong handler. Apache will then generate the originally requested
segment and return that back to Nginx.  Nginx will then cache the segment and
return it to the client.


\begin{figure}
    \includegraphics{latetransold_diagram.pdf}
    \caption{The initial late transmuxing setup}\label{fig:latetransold}
\end{figure}

The outcome of the initial tests of this setup were as expected. As it turns out
however, in its current form the setup has one big problem. When testing this
setup with more concurrent connections than the amount of processes that
Nginx is defined to use it results in deadlock. The reason for this is that the
\ipplong directive supplied by \usp spawns the range requests in a blocking way,
instead of in the non blocking way that Nginx normally spawns requests. This
causes the threads that spawn them to wait until they get a response from the
upstream server. However, since the upstream server is the same server, it will
only be able to do so when not all the threads are blocked. When using enough
concurrent client requests all available threads will get into the waiting
state, so no more threads are available to serve the requests that would get
them out of this state.

\section{Final implementation}
There are a couple of possible solutions to the deadlock problem, but by far the
easiest is moving the blocking code outside Nginx. In the final implementation
this is done by not only running Nginx on the proxy server, but also Apache. The
only task of Apache is to run the \ipplong code. This way all that Nginx does is
proxying and caching. Once a request of a client comes in, it will pass
through Nginx to Apache on the same server. Apache will then generate the range
requests and those will pass through Nginx again, so it can cache their
responses. Nginx will then send them to the storage server and then the response
from the storage will follow the chain back. And just like in the initial
implementation when a response passes a proxy in Nginx it will be cached. For a
schematic overview of this setup see figure \vref{fig:latetrans}.


\begin{figure}
    \includegraphics{latetrans_diagram.pdf}
    \caption{The final late transmuxing setup}\label{fig:latetrans}
\end{figure}







\chapter{Experiments}
This chapter will explain the experiments that have been done to compare the
\lt setup to other available setups. It will also show the results of these
experiments.

\section{Setup}

\subsection{Virtual machines}
To create the proposed setup a client, a proxy server and a storage server are
needed. This could be have been done by using actual physical servers for every
type of machine. However, to make the development process easier the different
machines where simulated using virtual machines. VirtualBox was used to create
the VMs. One VM was used as the proxy and a second one was used as the storage
server. The host operating system was used as the client. Since the machines
have to communicate with each other, a separate network interface was set up to
allow communication. The VMs have the same virtual hardware, they are both
allowed to use \SI{2}{\giga\byte} of RAM and one CPU core.

\subsubsection{Comcast}
Since the VMs will be running on the same machine there are almost no network
limitations. That is why artificial network throttling is needed to simulate
more realistic real world environments. Comcast is a command line tool that
provides a simple interface over standard Linux tools to throttle the
network~\autocite{comcast}. Using Comcast the bandwidth between the proxy server
and the storage server is throttled down to allow a maximum of
\mbps{100}.


\subsection{The different setups}
The VMs are six different setups that will be tested in a variety of tests.
These setups can be split into old setups and new setups and control setups.
The old setups consist of of the \cdn and \ipp setup, described in
\nameref{ch:problem}. The new setups consist of the two flavors of the \lt
setup. The one that only caches the responses to the range requests will be
called \lt-single and the one that also caches the responses to the client
\lt-double. The control setups are setups that are purely there to test the
effect of the actual caching the \lt and the \cdn setups. These setups have all
caching disabled, but the rest is the same and they will respectively be called
\lt-nocache \cdn-nocache.


\subsection{The tests}
The performance of each setup will be measured when downloading all the
segments of a whole video in the highest quality available in all the formats
supported by the \usp software. All the segments of the video in a specific
format will be referred to from now on as ``the video"

\subsubsection{Generating requests}
Since only one client is used, it is necessary that it generates a significant
amount of requests in a short time so it can be measured how the different
setups perform under different loads. Wrk is a HTTP benchmarking tool that will
be used to do this~\autocite{wrk}. It can be supplied with an URL, a duration
and the amount of concurrent connections. It will then fire requests for the
duration provided to that URL and afterwards it will output statistics about

Wrk also provides a scripting interface that allows for more complicated logic
for the requests to be sent. This interface is used by the testing setup to fire
requests for all the segments of the video. This is done by looping over the
contents of a file that contain all the URLs for the different segments of the
video in a specific format. The files for the different formats have been
generated by letting a player play the video from start to end and saving the
requests. The player used for this was the Unified Capture tool created by \usp,
which impersonates a player to save an online video to a file.

\subsubsection{Different requests for different formats}\label{sec:filediff}
Because the different \abs formats differ quite a bit in some aspects they also
need different amounts of different types of files to play the whole
video. There are four different types of files used by the formats. There are
playlist files, these tell the player what further files to request to actually
play the video. Furthermore there are video files, audio files and combined
video and audio files. Both \dash and \iss use separate video and audio files.
For our specific video both request 184 video files and 147 audio files. \hds
and \hls use files that contain both the video and audio for a segment, both use
184 of these. Some differences also exist in the amount of playlist files, \iss
and \hds need only one to play the video, \dash uses three and \hls uses five.
This information is not necessarily useful to set up the tests themselves, but
it might be useful when interpreting the results.

\subsubsection{Measuring performance}
Performance of a setup will be measured by a six kinds of characteristics. The
first three characteristics are simply measured by wrk. The first of these is
the amount of requests that are handled per second. The second is the amount of
Megabytes received per second and the third is the average time it takes for a
request to return a response. The fourth characteristic is the storage space the
cache directory takes up on the proxy server. The last two characteristics
involve the traffic between the proxy server and the storage server. That
traffic is measured in the amount of requests and the amount of bytes sent.
These are simply measured by parsing the log files on the storage server.

\subsubsection{Cache conditions}
Performance tests will be executed on the different setups in three different
cache conditions. The first one is where the cache is cold. The second one is
where the cache is already warm from downloading the video in the same format
that is getting requested. The last one is where the cache is warm from
downloading the same video but in another format than the one that is requested.

The tests for the first and the last situation simply try to download the video
once, as fast as possible. The tests for the second situation try to download
the video as often as possible in a 30 second time frame. The reason this
difference exists, is the first and last situation both change to the second
situation when the video has been downloaded once, since the cache has become
warm from downloading the video in that format.

Since the different cache conditions have no impact on the setups without
caching, the \ipp, the \cdn-nocache and the \lt-nocache setup have only been
executed for the first situation. This is done to reduce the time that is needed
for a test run, so more useful tests can be executed in the same time.





\section{Results}
The results of the different measurements are shown separately for the three
different cache conditions. The transfer speed and latency measurements are
plotted against the amount of concurrent connections. For the cache usage and
internal traffic measurements this is not the case, since the amount of
connections have no effect on these statistics. Not only the plots are shown,
but the important information that can be gathered from the plots is noted as
well.

\subsection{Cold cache}

\subsubsection{Transfer speed measurements}
In the figures~\ref{fig:first_time_mbps}
and~\ref{fig:first_time_requests_per_second}, that show the transfer speed
measurements, a couple of things should be noticed:
\begin{enumerate}
    \item
        The \cdn type setups perform best in almost all of the transfer
        speed measurements.

    \item
        All setups eventually reach a maximum transfer speed when the amount of
        concurrent connections is increased.

    \item
        The \cdn type setups already reach a maximum in transfer speed already
        after two connections.

    \item
        Unlike the other setups, the \ipp setup reacts very irregularly to
        an increasing amount of connections.

    \item
        Some setups behave quite similar to others. The \cdn type setups perform
        roughly the same and the caching \lt setups do this as well. The \ipp
        and the \lt-nocache setup share the maximum they reach and also perform
        quite the same with one connection. They differ in between the setups
        however.

    \item
        The maximum reached by the caching \lt setups is almost as the same as
        the maximum from the \cdn type setups, although it is reached only with
        more concurrent connections.

    \item
        Most setups perform similarly for the different formats when comparing
        MB/s, but when comparing requests per second they perform worse for \hds
        and \hls than for \dash and \iss.

\end{enumerate}

\input{plots/first_time_mbps.tex}
\input{plots/first_time_requests_per_second.tex}
\FloatBarrier


\subsubsection{Latency}
In figure~\ref{fig:first_time_latency_mean} is shown what the difference in
latency is and how the latency is influenced by increasing the connections. The
y axis is displayed with a log scale to better show the differences with a small
amount of concurrent connections. The things to notice here are:
\begin{enumerate}
    \item
        The \ipp setup again performs the worst and the \cdn type setups perform
        the best.

    \item
        With one connection the \cdn setups outperform all the other setups.

    \item
        With enough concurrent connections the caching \lt setups again perform
        about as good as the \cdn type setups.



\end{enumerate}

\input{plots/first_time_latency_mean.tex}

\subsubsection{Cache usage}
The cache usage measurements in figure~\ref{fig:first_time_cache_usage} show a
only a couple of important things:
\begin{enumerate}
    \item
        The setups that use no caching still use a small amount of storage for
        cache.

    \item
        The \lt-single setup and the \cdn setup use about the same amount of
        cache.

    \item
        The \lt-double setup uses almost double the amount of cache as the
        \lt-single and \cdn setup.
\end{enumerate}

\input{plots/first_time_cache_usage.tex}
\FloatBarrier

\subsubsection{Internal traffic}
The internal traffic statistics in figure~\ref{fig:first_time_internal_mb}
and~\ref{fig:first_time_internal_requests} show these things:
\begin{enumerate}
    \item
        The \ipp and \lt-nocache perform the same, as do the \cdn type setups
        and the caching \lt setups do this as well.

    \item
        In both statistics, the \ipp and \lt-nocache perform worst.

    \item
        When comparing the \cdn type setups and the caching \lt setups perform
        about the same when comparing the amount of MB that is internally sent.

    \item
        The \ipp and \lt-nocache setups send more than double the amount of
        internal requests the caching \lt setups send.

    \item
        The caching \lt setups send quite a bit more internal requests than the
        \cdn type setups. For \dash and \iss videos they send about double the
        amount of requests and for \hds and \hls about three times the amount of
        requests.

    \item
        The amount of requests for \hds and \hls is higher than the amount of
        requests for \dash and \iss is higher when looking at the \ipp and \lt
        type setups, but it is lower when looking at the \cdn type setups.


\end{enumerate}

\input{plots/first_time_internal_mb.tex}
\input{plots/first_time_internal_requests.tex}

\FloatBarrier

\subsection{Cache filled with the same format}
Apart from explaining what the plots show, this section will also compare them
to the plots of equivalent date gathered with a cold cache.


\subsubsection{Transfer speed measurements}
The figures~\ref{fig:second_time_mbps}
and~\ref{fig:second_time_requests_per_second}, that show the transfer speed
statistics, show this information:
\begin{enumerate}
    \item
        The \lt-double and the \cdn setup perform the roughly the same in all
        cases.

    \item
        \dash and \iss perform roughly the same and \hds and \hls perform mostly
        the same as well for all setups.

    \item
        When increasing the amount of connections the \lt-double and \cdn setup
        first perform better, but at some point the performance will drop
        drastically again.

    \item
        The \lt-single setup performs significantly worse, especially with a low
        amount of concurrent connections.

    \item
        All setups perform better than the same setups with a cold cache,
        although \lt-single only performs slightly better compared to the gains
        that are achieved by the \lt-double and \cdn setups.

\end{enumerate}
\input{plots/second_time_mbps.tex}
\input{plots/second_time_requests_per_second.tex}
\FloatBarrier

\subsubsection{Latency}
All the observations that were made for the transfer speeds also hold true for
latency, which can be seen in figure~\ref{fig:second_time_latency_mean}. The
only difference is that with two connections the confidence intervals for \cdn
and \lt-double are exceptionally large for some of the formats.
\input{plots/second_time_latency_mean.tex}
\FloatBarrier

\subsubsection{Cache usage}
The only thing to gather from figure~\ref{fig:second_time_cache_usage} is that
the cache usage is exactly the same as the cache usage with a cold cache.
\input{plots/second_time_cache_usage.tex}

\subsubsection{Internal traffic}
The figures~\ref{fig:second_time_internal_mb}
and~\ref{fig:second_time_internal_requests} show quite clearly that no internal
requests are sent for any of the setups.
\input{plots/second_time_internal_mb.tex}
\input{plots/second_time_internal_requests.tex}


\FloatBarrier
\subsection{Cache filled with another format}
This section will show the statistics when running the different setups after
the cache has been filled with the video in another format.
Important to note that following plots are a bit different from the previous
ones. These plots not only compare the performance of the setups for the
different video formats, but they also compare that to the video format that was
loaded in the cache. To plot this out in a nice and easy way there are also
plots that supposedly show a format being requested after itself. These plots
are left empty however, since this would result in the same data as with a cache
filled with the same format.


\subsubsection{Transfer speed measurements}
In the figures~\ref{fig:after_other_mbps}
and~\ref{fig:after_other_requests_per_second}, the diagonals are left empty
because as mentioned before, since this would show data that has already been
shown. However, in the other plots some interesting things can be noticed:
\begin{enumerate}
    \item
        The amount of requests per second are quite a lot lower for \hds and
        \hls. This was also the case when requesting videos with a cold cache.

    \item
        The \lt-double setup performs better than the \lt-single in all of the
        cases. The \cdn setup performs roughly the same however.

    \item
        Both the \lt setups perform better than the \cdn setup in all of the
        cases. They also perform better than with a cold cache.

    \item
        When comparing received MB/s for the \lt setups it seems that the order
        in which two formats are requested does not matter much. Complementary
        plots are very similar, for instance requesting \dash after \iss
        performs about as well as requesting \iss after \dash

    \item
        When comparing received MB/s requesting \hls after \hds, \hds after
        \hls, \dash after \iss and \iss after \dash all perform roughly the same
        for the \lt setups. And any of the other orders perform roughly the same
        as well. The first set of these perform best for both the \lt setups.

    \item
        When using five concurrent connections all the setups reach a maximum in
        both performance measures.

\end{enumerate}
\input{plots/after_other_mbps.tex}
\input{plots/after_other_requests_per_second.tex}
\FloatBarrier

\subsubsection{Latency}
The latency plot in figure~\ref{fig:after_other_latency_mean} mostly shows the
same characteristics as the requests per second plot in
figure~\ref{fig:after_other_requests_per_second}:
\begin{enumerate}
    \item
        Again the \lt-double performs better than the \lt-single, which then
        outperforms the \cdn setup again.

    \item
        The latency for the \cdn setup is roughly the same as it was when
        requesting the same format with a cold cache.

    \item
        The \lt setups perform better than they did when the cache was cold.

    \item
        \hds and \hls perform worse than \iss and \dash.

    \item
        The requesting orders that perform best are again \iss after \dash and
        \dash after \iss.

    \item
        The requesting orders that perform worse are \hls and \hds after \dash
        or \iss.

\end{enumerate}

\input{plots/after_other_latency_mean.tex}
\FloatBarrier



\subsubsection{Cache usage}
First, to reiterate, the bars that represent requesting content after itself
are empty not because they are zero, but because these are the results shown in
the previous subsection. The other bars show this however:

\begin{enumerate}
    \item The cache usage for the \lt-single setup is roughly the same as it was
        with a cold cache. For the \lt-double setup the cache usage increased by
        about half and for the \cdn setup it almost doubled.

    \item The \lt-single setup uses the least amount of cache in all cases. The
        \cdn setup uses about two times as much and the \lt-double setup about
        three times as much.

    \item In a some of the cases the \lt-single setup uses a bit more cache than
        it did when requesting the same format with a cold cache. These cases
        are \hds and \hls after \dash and \iss and the other way around.  When
        requesting \dash after \iss or \hls after \hds or both the other way
        around the cache usage stays the same.

\end{enumerate}
\input{plots/after_other_cache_usage.tex}
\FloatBarrier

\subsubsection{Internal traffic}
The information that can be gathered from the internal traffic statistics is
this:
\begin{enumerate}
    \item
        The amount internal traffic with the \cdn setup is the same as it was
        for the same format with a cold cache.

    \item
        There is no difference in the amount of internal traffic between the
        two \lt setups.

    \item
        Looking only at the \lt setups, when requesting \dash after \iss, \iss
        after \dash or \hds after \hls no internal data is sent.

    \item Looking only at the \lt setups, when requesting \hls after \hds only a
        very small amount of requests is sent internally.

    \item When requesting \hds or \hls after \dash or \iss, with the \lt setups,
        about 150 internal requests are sent. When \dash or \iss are requested
        after \hds or \hls about 180 internal requests are sent.  The amount of
        data that is sent in all these cases is all roughly the same.

    \item
        Comparing the \lt setups with the \cdn setup in the cases mentioned in
        the previous item. When \hds or \hls are requested, the amount of
        requests is about the same for both type of setups and when requesting
        \dash or \iss, the amount of requests for the \lt setups is about half
        of that of the \cdn setup. When comparing the amount of data sent
        internally, the \lt setups send a little less than a sixth of the data
        internally than the \cdn setup does.

\end{enumerate}

\input{plots/after_other_internal_mb.tex}
\input{plots/after_other_internal_requests.tex}



\FloatBarrier







\chapter{Discussion}
This chapter tries to explain and relate the many results gathered from the
experiments described in the previous chapter. It also suggests opportunities for
future research subjects.

\section{Explanation of the results}
Some results can easily be explained easily, some others need some more
work. This is why the full explanation is split into smaller sections, which
explain the results in an easy order.

\subsection{DASH and ISS versus HLS and HDS}\label{sec:audiovs}
A red line through all the measurements is that \hls and \hds perform
quite similar and \dash and \iss do this as well. Even when all of the four
formats perform pretty similar, the pairs of formats perform even similar.
This similarity or difference can easily be explained by the type of files the
different formats use. \dash and \iss use
separate files for audio and video, while \hls and \hds combine those two. This
means only about half the amount of files is needed when streaming \hls or \hds
when compared to \dash or \iss, see section~\ref{sec:filediff} for more
information.

Because of this difference in amount of requests for the different formats,
comparing them using handled requests per second is unfair. A good example of
this can be found by looking at the plots for handled requests per second and
received MB/s. While the different formats perform quite similar when looking at
received MB/s, \hls and \hds perform about half as bad as \dash and \iss when
looking at requests per second. This can only mean that the requests for \hds
and \hls are, on average, about twice as large as the requests for \dash and
\iss. Which in turn can be easily explained by separate requests for audio,
since audio files are much smaller than video files. The same holds true for
comparing latency, since the \hls and \hds have to do more work for a request it
is not really fair to compare them against \dash and \iss. While the
measurements for requests per second and latency clearly not fair when comparing
different formats, it is still very useful to compare the different setups.

\subsection{Internal traffic with a cold cache}
Findings that easy explained are the identical internal traffic statistics for
pairs of setups. The reason that the different \cdn type setups are the same is
this regard is caused by the cold cache. Every time the \cdn setup requests a
segment from the storage this is cached, so it can be used by a future request
that is exactly the same. However, since every segment is only requested once,
the \cdn setup only builds the cache, but it is not able to use the stored data
yet. This is why internal traffic statistics should not be influenced by this
caching. The same holds true when comparing \lt-single \lt-double, since all
that is difference between those setups is that the \lt-double setup caches its
response to the client, which is again different for every request. As a last
pair, the \ipp and the \lt-nocache, perform the same as well. This is most
likely caused by the fact that these setups are almost identical, except that
one runs on Nginx directly and the other on Apache, which is proxied through
Nginx.

% TODO: Add a log count snippet somewhere
The more interesting information lies in the fact that the \lt-nocache does not
perform the same as the other \lt setups, in contrast to the \cdn and the
\cdn-nocache setup. This means that the subrequest caching employed by the to
caching \lt setups has an advantage even when the clients requests are all
different, which means cache hits occur. This means that some of the range
requests that get spawned by the \ipp directive are the same for different
segments. When looking at the logs for one of these setups, it quickly becomes
clear that it always requests byte ranges at the beginning and the end of the
video file. These sections most likely contain data about the video that is
necessary to construct the different formats.

What is also interesting is that, while the differences in requests are quite
large between the different pairs, the differences in sent MB are quite small.
Between the \cdn setups and the caching \lt setups no difference, this is likely
caused by the fact that both have to request the entire video from the storage.
For the \ipp and \lt-single setups the large difference in amount of requests,
when compared to the other setups, and the small difference in amount of MB can
be explained by the fact that the file header and footer that it requests are
very small, compared to the actual segment of video.

\subsection{Cache usage with a cold cache}
When measuring the cache usage with a cold cache, the different usages can be
quite easily explained. First of all, the none caching setups use almost no
storage for cache, but they still use some. This is caused by the way Nginx
its cache works, it uses directory layers to store the
entries~\autocite{nginxcachelevels}. These directories itself also take up
some space. Secondly the \lt-single and the \cdn setup use about the same amount
of cache, because they both store the entire video. They do store it in a
different format however, the \lt-single setup stores it in byte ranges of the
original mp4 and \cdn setup stores it in converted segments. Apparently this
difference in format does not change the size much. Lastly, the \lt-double setup
uses about double the amount of cache that the \lt-single and the \cdn setup
use. This is simply because it caches the video in both of the formats that
these setups cache them in, which easily explains the increase in cache usage.

\subsection{Transfer speed and latency with a cold cache}
One important observation from the transfer speed plots is that that all the
different setups reach a maximum. This maximum is at about \Mbps{12} for the
caching \lt setups and the \cdn setups and it is at about \Mbps{10} for the
\ipp and \lt-nocache setup. These two maxima are mostly caused by the bandwith
restriction of \mbps{100} on internal traffic. This can easily be shown for the
first set of setups by showing this $\mbps{100} = \Mbps{12.5} \approx
\Mbps{12}$. The lower maximum shown for the other set of setups can than be
explained by the higher amount of internal traffic for those setups, shown in
figure~\ref{fig:first_time_internal_mb}.

The difference in transfer speed between the different setups can mostly be
explained by looking at the internal traffic statistics. The \cdn type setups
both sent the least internal requests and the least data, which is why they
perform best. The caching \lt setups sent the same amount of data, but they sent
more requests, which means more overhead. This overhead is apparently most
noticeable when doing a small amount of concurrent requests. Lastly the \ipp and
\lt-nocache setups probably performed worst because they both had the most
amount of internal requests and they sent the most data internally.

One thing that is not explained by the internal traffic is why the \ipp setup
performs worse, both in transfer speed and latency, than the \lt-nocache setup
when using more than one request.  Since they are basically the same, there
normally should not be such a large difference. One reason could be that the
blocking nature of the \ipp directive, described in
section~\ref{sec:initimplement}, causes Nginx to slow down. By removing the
blocking code from Nginx, although originally meant to remove a dead lock, this
problem can be avoided. Something else that is not explained is the difference
in performance between the two caching \lt setups. This is also a phenomenon in
some of the other plots and that is why it will be discussed in
section~\ref{sec:weirdstuff}.


\subsection{Results from a cache filled with the same
format}\label{sec:discusssame}
The most easy results to explain for the test condition where the cache is
filled with the same format are the cache usage and the internal traffic
results. No internal traffic has taken place for any of the caching setups. No
internal traffic has taken place, because the cache already contained all the
data needed to fulfil the requests for the video segments. The \lt-double and
the \cdn setup both have the responses that they send to the clients saved in
the cache and the \lt-single setup already has the range requests it needs to
generate the segment in the requested format.

The reason why all caching setups perform better, transfer speed and latency
wise, with their cache filled is also quite clear. No time is spent on internal
traffic, so any delay that was caused there in the cold cache tests is now gone.
This is the case for all the three caching setups. For the \lt-double setup and
the \cdn setup, another phase is skipped as well because of the caching. This
phase is the video conversion. All these two setups have to do is get the cache
entry and send it back to the client. The \lt-single still needs to do the
conversion phase, it has to get the range requests from cache and transform them
into a segment. This is probably why the \cdn and \lt-double setup perform very
similar and why the \lt-single setup performs better than with a cold cache, but
not as good as the other two setups.

The one thing that has not been explained yet is the drop in performance for the
\lt-double and \cdn setup when adding more connections. The reason for this drop
most likely has something to do with overloading the server. As mentioned
before, the servers used in the tests are not very powerful and is quite likely
the cache server could not handle the load of so many concurrent connections
that all finished so quickly.

\subsection{Results from a cache filled with the another format}
When looking at the results gathered when the cache is filled with another
format than the requested format the internal traffic measurements explain most
of the other results as well. Which is why the internal traffic measurements
will be explained first. The most obvious part of these measurements are
the measurements for the \cdn setup. These measurements are identical to the
results obtained when testing with a cold cache. The reason for this is that the
\cdn setup has no use for the segments it has in cache, since segments in
another format are requested and the ones in cache do not match them. This means
that it still has to ask the storage server for the segments and wait until they
are returned from there. This is probably also the reason why the measurements
for the two \lt setups are the same. The only difference between the two is
that the \lt-double caches the responses it sends to the client. However, in
this case those cached responses should have no use, since another format is
requested.

The more interesting fact about the internal traffic measurements is that the
\lt setups cause a lot less internal traffic when compared to the results from
the cold cache. All of them have less internal requests and less data sent, some
orders even have no requests sent at all. The measurements where no requests are
sent mean all the data necessary for the generation of the fragment was all
ready in the cache. When looking at \dash and \iss, this goes both ways. When
looking at \hds and \hls, it can be seen that a very small amount of requests
still needs to be sent when requesting \hls after \hds, but not the other way
around. When looking at other orders, a small amount of data still needs to be
sent.  However, this small amount of data accounts for a large part of the
requests.  The most probable reason for this is that these extra requests are
caused by the file differences mentioned in section~\ref{sec:filediff}. The
amount of video files is the same as the amount of combined audio/video files,
both of them 184. Which is why the video ranges would match each other.
However, the amount of audio files needed for \dash and \iss is around 147 and
the amount of combined audio/video files is still 184. This probably causes
range mismatches on the range requests targeted at the audio file, since it gets
divided in a different amount of pieces. The numbers seem to match up when
looking at the amount of internal requests that still get sent, about 150 for
\dash and \iss and about 180 for \hds and \hls. This would also explain why the
amount of data sent internally is so small, compared to the amount of requests,
since audio is pretty small when compared to video.

This
would explain why the MB/request ratio is so low and why the different all this
and why they only occur in these orders, since the length of the audio segments
would only be different in these cases.

The one thing that is not explained by this is is the very small amount of
requests that needs to be sent when requesting \hls after \hds. It is not
directly clear what causes this. One thing that could be the cause is the large
amount of playlist files \hls uses. It could very well be the case that one or
more of those only need a small range of the video file, which does not match
the ranges needed to generate the playlist for \hds.

When looking at the cache usage it is seen that the \lt-single setup uses almost
no more than it did after running on a cold cache. This can easily be explained
by the fact that the amount of data sent internally was very little and that is
the only thing that the \lt-single setup caches. The \cdn setup uses about
double as much as it did with a cold cache, this can also easily be explained by
the fact that it sent about the same amount of internal data, which is also
cached again. The \lt-double setup is increased by about half and this can again
be explained by the fact that it caches all the data that the other two setups
cache, which put together is about half as much.

When looking at the transfer rate and latency data it comes as no surprise that
the \cdn setup performs about the same as it did with a cold cache, since it
should essentially do the same. What is very interesting though is the
improvements the \lt setups make over the \cdn setup and when they would run
with a cold cache. These improvements can only be explained by the fact that so
little data has to be sent internally. The request orders where (almost) no data
had to be sent internally eventually increases about five fold in performances
for the \lt-single setup and about seven fold for the \lt-double setup. For the
request orders where some data had to be sent internally, the setups still have
an increase of about four and five fold respectively. The latency measured is
also much lower for the \lt setups than for the \cdn setup in all the request
orders.


\subsection{The speed and latency improvement of double caching over single
caching}\label{sec:weirdstuff}
The one thing that has not been explained yet is the difference in transfer
speed and latency between the two caching \lt setups. The positive effects of
the second cache in the \lt-double setup have a clear explanation when the cache
is filled with the same format as requested, which was explained in
section~\ref{sec:discusssame}. However, both with an empty cache and a cache
filled with another format the \lt-double setup clearly outperforms the
\lt-single setup. In these cases this performance gain cannot be explained by
getting actual cache hits in its extra cache, since none of the clients requests
match earlier ones. The only explanation left is that the actual caching of the
responses has a beneficial side effect for these performance measures. It could
be that Nginx does something clever, that makes proxying responses faster when
they are allowed to be saved to disk. It is very unclear however, what exactly
causes this weird but beneficial behaviour.




\section{Future work}
Some of the results and findings in this thesis present possibilities for future
research. First of all, the unexpected performance differences between the
caching \lt setups that occurred just when caching the files. Apart from the
natural difference when being able to serve from cache. Two other options for
future research are based on the shortcomings found when creating an
implementation. The first of these is improving the current suboptimal methods of
caching range requests. For instance by creating the theoretical version
described in section~\ref{sec:cachingrange}. Something else that could be
improved in future research would be the \ipplong directive. In its current
blocking state it limits Nginx its performance and it would be interesting to
see that changed, so no second HTTP server would have to be run to implement the
\lt setup. A last and logical direction for future research is the testing of
the two caching \lt setups at a large scale, with lots of servers and lots of
different videos. It could be that there are some issues that only occur when
deploying at a large scale, which could require some extra tweaking of the
setups.








\chapter{Conclusion}
Streaming video has become a large part of the internet. Currently a well
known server setup for streaming video is by using a \cdn, which caches popular
videos for quick access. One of the problems with this approach is that requires
caching of the videos in every format that is supported. This thesis
investigated the possibility of instead caching the source mp4 of the different
formats which could then be converted to the each format, when requested.
To accomplish this two different setups have been developed, one which caches
only the source mp4 and another which also caches the different formats after
conversion has taken place.

To compare the different setups, their performance was measured under three
different cache conditions. One where the cache was empty, one where the cache
was filled by requesting the same format as the one that was tested and one
where it was filled by requesting a different format. Both new setups perform a
little bit worse than or equal to the \cdn setup when the cache is cold.
However, when the cache is filled with a different format they outperformed the
\cdn setup in speed and latency by as much as four to seven times under heavy
load. The internal traffic between the storage server and the proxy server is
also a lot lower in this case and the setup that only caches the source mp4 used
about half storage for cache the \cdn. The last condition was when the cache is
filled with the same format, here both setups perform better than they did with
a cold cache, but only the setup that caches twice performs speed and latency
wise on par with the \cdn setup.

All in all, the new setups are valuable additions to currently known setups. The
setup that caches twice, when comparing to the \cdn setup, is faster or as
fast at serving content and generates a lot less internal traffic. The only
negative aspect is that this setup uses more storage for cache. The version that
caches once still has the internal traffic advantages that the other setup has,
but it performs worse than the \cdn in speed, when requesting the same format
multiple times, which is not uncommon. However, it can serve multiple formats
with less cache usage than the \cdn setup. This means that depending on the
main requirement, speed or cache usage, a fitting setup can be chosen. Which
could make hosting videos cheaper, faster and more environment friendly.





\printbibliography[heading=bibintoc]{}

\printglossaries{}


\end{document}
